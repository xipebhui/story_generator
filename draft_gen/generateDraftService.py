#!/usr/bin/env python3
"""
å‰ªæ˜ è‰ç¨¿ç”ŸæˆæœåŠ¡
ç»¼åˆéŸ³é¢‘ã€å¤šå›¾ç‰‡ã€è½¬åœºå’Œç‰¹æ•ˆçš„å®Œæ•´è‰ç¨¿ç”ŸæˆæœåŠ¡
"""

import os
import sys
import json
import shutil
import random
import argparse
import zipfile
import logging
from typing import List, Optional
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# å¯¼å…¥å®‰å…¨æ‰“å°å‡½æ•°
try:
    from utils import safe_print
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œå®šä¹‰ä¸€ä¸ªç®€å•çš„ safe_print
    import platform
    def safe_print(message: str, file=None):
        if platform.system() == 'Windows':
            message = message.replace('âœ…', '[OK]').replace('âŒ', '[ERROR]').replace('âš ï¸', '[WARNING]')
        try:
            print(message, file=file)
        except UnicodeEncodeError:
            message_ascii = message.encode('ascii', 'replace').decode('ascii')
            print(message_ascii, file=file)

from pydub import AudioSegment
from models.draft_models import (
    Draft, Track, Segment, VideoMaterial, AudioMaterial, Materials,
    Timerange, Clip, Transition, VideoEffect, Keyframe, CommonKeyframe
)
from models.draft_effects_library import (
    Transitions, VideoEffects, SpeedInfo, CanvasInfo, MaterialAnimationInfo
)
from jianying_subtitle_service import get_subtitle_service
import uuid

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def gen_upper_uuid():
    """ç”Ÿæˆå¤§å†™UUID"""
    return str(uuid.uuid4()).upper().replace('-', '')


class DraftGeneratorService:
    """å‰ªæ˜ è‰ç¨¿ç”ŸæˆæœåŠ¡"""

    def __init__(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        self.supported_image_formats = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp']
        self.supported_audio_formats = ['.mp3', '.wav', '.m4a', '.aac', '.flac', '.ogg']

        # å¯ç”¨çš„è½¬åœºåˆ—è¡¨ï¼ˆç”¨äºéšæœºé€‰æ‹©ï¼‰
        self.available_transitions = [
            Transitions.PUSH_IN_II,
            Transitions.SLIDE_RIGHT
        ]

        # å¯ç”¨çš„ç‰¹æ•ˆåˆ—è¡¨ï¼ˆç”¨äºéšæœºé€‰æ‹©ï¼‰
        self.available_effects = [
            VideoEffects.SNOW
        ]
        
        # é»˜è®¤å›¾ç‰‡ç¼©æ”¾æ¯”ä¾‹
        self.default_image_scale = 1.8

    def get_audio_duration_ms(self, audio_path: str) -> int:
        """è·å–éŸ³é¢‘æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰"""
        audio = AudioSegment.from_file(audio_path)
        return len(audio)

    def scan_images(self, images_dir: str) -> List[str]:
        """æ‰«æç›®å½•ä¸‹çš„æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶"""
        images = []
        for file_path in Path(images_dir).iterdir():
            if file_path.is_file() and file_path.suffix.lower() in self.supported_image_formats:
                images.append(str(file_path))

        # æŒ‰æ–‡ä»¶åæ’åº
        images.sort()
        return images

    def create_keyframes_for_segment(self, segment_duration_us: int, segment_index: int, image_scale: float = 1.8) -> List[CommonKeyframe]:
        """
        ä¸ºç‰‡æ®µåˆ›å»ºå…³é”®å¸§åŠ¨ç”»
        
        Args:
            segment_duration_us: ç‰‡æ®µæ—¶é•¿ï¼ˆå¾®ç§’ï¼‰
            segment_index: ç‰‡æ®µç´¢å¼•ï¼ˆç”¨äºå†³å®šåŠ¨ç”»æ–¹å‘ï¼‰
            image_scale: å›¾ç‰‡ç¼©æ”¾æ¯”ä¾‹ï¼ˆé»˜è®¤1.8ï¼‰
            
        Returns:
            å…³é”®å¸§åˆ—è¡¨
        """
        keyframes = []
        
        # æ ¹æ®ç‰‡æ®µç´¢å¼•å†³å®šåŠ¨ç”»ç±»å‹
        if segment_index % 2 == 0:
            # å¶æ•°ç´¢å¼•ï¼šä»ä¸Šåˆ°ä¸‹ç§»åŠ¨
            # Yè½´ä½ç½®å…³é”®å¸§
            keyframes.append(CommonKeyframe(
                id=gen_upper_uuid(),
                property_type="KFTypePositionY",
                keyframe_list=[
                    Keyframe(
                        id=gen_upper_uuid(),
                        time_offset=0,
                        values=[0.8]  # èµ·å§‹ä½ç½®ï¼šä¸Šæ–¹
                    ),
                    Keyframe(
                        id=gen_upper_uuid(),
                        time_offset=segment_duration_us,
                        values=[-0.8]  # ç»“æŸä½ç½®ï¼šä¸‹æ–¹
                    )
                ]
            ))
        else:
            # å¥‡æ•°ç´¢å¼•ï¼šä»ä¸‹åˆ°ä¸Šç§»åŠ¨
            # Yè½´ä½ç½®å…³é”®å¸§
            keyframes.append(CommonKeyframe(
                id=gen_upper_uuid(),
                property_type="KFTypePositionY",
                keyframe_list=[
                    Keyframe(
                        id=gen_upper_uuid(),
                        time_offset=0,
                        values=[-0.8]  # èµ·å§‹ä½ç½®ï¼šä¸‹æ–¹
                    ),
                    Keyframe(
                        id=gen_upper_uuid(),
                        time_offset=segment_duration_us,
                        values=[0.8]  # ç»“æŸä½ç½®ï¼šä¸Šæ–¹
                    )
                ]
            ))
        
        # æ·»åŠ Xè½´ä½ç½®å…³é”®å¸§ï¼ˆä¿æŒä¸å˜ï¼‰
        keyframes.append(CommonKeyframe(
            id=gen_upper_uuid(),
            property_type="KFTypePositionX",
            keyframe_list=[
                Keyframe(
                    id=gen_upper_uuid(),
                    time_offset=0,
                    values=[0.0]
                ),
                Keyframe(
                    id=gen_upper_uuid(),
                    time_offset=segment_duration_us,
                    values=[0.0]
                )
            ]
        ))
        
        # æ·»åŠ ç¼©æ”¾å…³é”®å¸§ï¼ˆä½¿ç”¨ä¼ å…¥çš„ç¼©æ”¾æ¯”ä¾‹ï¼‰
        keyframes.append(CommonKeyframe(
            id=gen_upper_uuid(),
            property_type="KFTypeScaleX",
            keyframe_list=[
                Keyframe(
                    id=gen_upper_uuid(),
                    time_offset=0,
                    values=[image_scale]
                ),
                Keyframe(
                    id=gen_upper_uuid(),
                    time_offset=segment_duration_us,
                    values=[image_scale]
                )
            ]
        ))
        
        # æ·»åŠ æ—‹è½¬å…³é”®å¸§ï¼ˆä¿æŒä¸æ—‹è½¬ï¼‰
        keyframes.append(CommonKeyframe(
            id=gen_upper_uuid(),
            property_type="KFTypeRotation",
            keyframe_list=[
                Keyframe(
                    id=gen_upper_uuid(),
                    time_offset=0,
                    values=[0.0]
                ),
                Keyframe(
                    id=gen_upper_uuid(),
                    time_offset=segment_duration_us,
                    values=[0.0]
                )
            ]
        ))
        
        return keyframes

    def select_random_images(self, all_images: List[str], total_duration_ms: int, image_duration_ms: int) -> List[str]:
        """
        æ ¹æ®éŸ³é¢‘æ—¶é•¿å’Œå›¾ç‰‡æ˜¾ç¤ºæ—¶é•¿ï¼Œéšæœºé€‰æ‹©å¹¶æ’åˆ—å›¾ç‰‡

        Args:
            all_images: æ‰€æœ‰å¯ç”¨çš„å›¾ç‰‡è·¯å¾„
            total_duration_ms: æ€»æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰
            image_duration_ms: æ¯å¼ å›¾ç‰‡æ˜¾ç¤ºæ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰

        Returns:
            é€‰ä¸­çš„å›¾ç‰‡è·¯å¾„åˆ—è¡¨ï¼ˆå¯èƒ½æœ‰é‡å¤ï¼‰
        """
        if not all_images:
            return []

        # è®¡ç®—éœ€è¦å¤šå°‘å¼ å›¾ç‰‡
        num_images_needed = total_duration_ms // image_duration_ms
        if total_duration_ms % image_duration_ms > 0:
            num_images_needed += 1

        selected_images = []

        # å¦‚æœå›¾ç‰‡æ•°é‡ä¸å¤Ÿï¼Œéœ€è¦é‡å¤ä½¿ç”¨
        if len(all_images) >= num_images_needed:
            # éšæœºé€‰æ‹©ä¸é‡å¤çš„å›¾ç‰‡
            selected_images = random.sample(all_images, num_images_needed)
        else:
            # éœ€è¦é‡å¤ä½¿ç”¨å›¾ç‰‡
            # å…ˆç”¨å®Œæ‰€æœ‰å›¾ç‰‡
            selected_images = all_images.copy()
            random.shuffle(selected_images)

            # è¡¥å……å‰©ä½™çš„
            remaining = num_images_needed - len(all_images)
            for _ in range(remaining):
                selected_images.append(random.choice(all_images))

        return selected_images

    def generate_draft(
            self,
            images_dir: str,
            audio_path: str,
            image_duration_seconds: float,
            video_title: str,
            output_dir: str,
            enable_transitions: bool = True,
            enable_effects: bool = True,
            enable_keyframes: bool = True,  # é»˜è®¤å¯ç”¨å…³é”®å¸§
            image_scale: float = 1.8,  # å›¾ç‰‡ç¼©æ”¾æ¯”ä¾‹
            random_seed: Optional[int] = None
    ) -> str:
        """
        ç”Ÿæˆå‰ªæ˜ è‰ç¨¿

        Args:
            images_dir: å›¾ç‰‡ç›®å½•è·¯å¾„
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            image_duration_seconds: æ¯å¼ å›¾ç‰‡æ˜¾ç¤ºæ—¶é•¿ï¼ˆç§’ï¼‰
            video_title: è§†é¢‘æ ‡é¢˜ï¼ˆç”¨ä½œè‰ç¨¿ç›®å½•åï¼‰
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
            enable_transitions: æ˜¯å¦å¯ç”¨è½¬åœº
            enable_effects: æ˜¯å¦å¯ç”¨ç‰¹æ•ˆ
            enable_keyframes: æ˜¯å¦å¯ç”¨å…³é”®å¸§åŠ¨ç”»ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
            image_scale: å›¾ç‰‡ç¼©æ”¾æ¯”ä¾‹ï¼ˆé»˜è®¤1.8ï¼‰
            random_seed: éšæœºç§å­ï¼ˆç”¨äºå¤ç°ç»“æœï¼‰

        Returns:
            ç”Ÿæˆçš„è‰ç¨¿ç›®å½•è·¯å¾„
        """

        # è®¾ç½®éšæœºç§å­
        if random_seed is not None:
            random.seed(random_seed)

        print(f"\n{'=' * 60}")
        print(f"å¼€å§‹ç”Ÿæˆå‰ªæ˜ è‰ç¨¿")
        print(f"è§†é¢‘æ ‡é¢˜: {video_title}")
        print(f"{'=' * 60}\n")

        # 1. éªŒè¯è¾“å…¥
        if not os.path.exists(images_dir):
            raise ValueError(f"å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {images_dir}")
        if not os.path.exists(audio_path):
            raise ValueError(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")

        # 2. æ‰«æå›¾ç‰‡
        print("æ‰«æå›¾ç‰‡æ–‡ä»¶...")
        all_images = self.scan_images(images_dir)
        if not all_images:
            raise ValueError(f"ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶: {images_dir}")
        print(f"æ‰¾åˆ° {len(all_images)} å¼ å›¾ç‰‡")

        # 3. è·å–éŸ³é¢‘æ—¶é•¿
        print("\nåˆ†æéŸ³é¢‘æ–‡ä»¶...")
        audio_duration_ms = self.get_audio_duration_ms(audio_path)
        audio_duration_us = audio_duration_ms * 1000  # è½¬æ¢ä¸ºå¾®ç§’
        print(f"éŸ³é¢‘æ—¶é•¿: {audio_duration_ms / 1000:.2f} ç§’")

        # 4. è®¡ç®—å›¾ç‰‡é…ç½®
        image_duration_ms = int(image_duration_seconds * 1000)
        image_duration_us = image_duration_ms * 1000  # è½¬æ¢ä¸ºå¾®ç§’

        # 5. é€‰æ‹©å›¾ç‰‡
        print(f"\né€‰æ‹©å›¾ç‰‡ï¼ˆæ¯å¼ æ˜¾ç¤º {image_duration_seconds} ç§’ï¼‰...")
        selected_images = self.select_random_images(all_images, audio_duration_ms, image_duration_ms)
        print(f"å°†ä½¿ç”¨ {len(selected_images)} å¼ å›¾ç‰‡")

        # 6. åˆ›å»ºè¾“å‡ºç›®å½•
        draft_dir = os.path.join(output_dir, video_title)
        materials_dir = os.path.join(draft_dir, "materials")
        os.makedirs(materials_dir, exist_ok=True)
        print(f"\nåˆ›å»ºè¾“å‡ºç›®å½•: {draft_dir}")

        # 7. å¤åˆ¶ç´ ææ–‡ä»¶
        print("\nå¤åˆ¶ç´ ææ–‡ä»¶...")

        # å¤åˆ¶éŸ³é¢‘
        audio_filename = f"audio_{os.path.basename(audio_path)}"
        audio_dest = os.path.join(materials_dir, audio_filename)
        shutil.copy(audio_path, audio_dest)
        audio_relative_path = f"##_draftpath_placeholder_0E685133-18CE-45ED-8CB8-2904A212EC80_##/materials/{audio_filename}"
        print(f"å¤åˆ¶éŸ³é¢‘: {os.path.basename(audio_path)}")
        
        # æ£€æŸ¥å¹¶å¤åˆ¶å­—å¹•æ–‡ä»¶
        subtitle_service = get_subtitle_service()
        audio_dir = os.path.dirname(audio_path)
        audio_name = Path(audio_path).stem
        srt_path = os.path.join(audio_dir, f"{audio_name}.srt")
        srt_dest = None
        
        # éªŒè¯å­—å¹•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        srt_file_path = None
        if subtitle_service.validate_subtitle_file(audio_path):
            srt_filename = f"subtitle_{os.path.basename(srt_path)}"
            srt_dest = os.path.join(materials_dir, srt_filename)
            shutil.copy(srt_path, srt_dest)
            print(f"å¤åˆ¶å­—å¹•: {os.path.basename(srt_path)}")
            # ä¿å­˜åŸå§‹srtè·¯å¾„ç”¨äºåç»­å¤„ç†
            srt_file_path = srt_path

        # å¤åˆ¶å›¾ç‰‡
        image_relative_paths = []
        for i, image_path in enumerate(selected_images):
            ext = os.path.splitext(image_path)[1]
            image_filename = f"image_{i:03d}{ext}"
            image_dest = os.path.join(materials_dir, image_filename)
            shutil.copy(image_path, image_dest)
            relative_path = f"##_draftpath_placeholder_0E685133-18CE-45ED-8CB8-2904A212EC80_##/materials/{image_filename}"
            image_relative_paths.append(relative_path)
            if i < 10:  # åªæ˜¾ç¤ºå‰10ä¸ª
                print(f"å¤åˆ¶å›¾ç‰‡ {i + 1}: {os.path.basename(image_path)}")
        if len(selected_images) > 10:
            print(f"... å’Œå…¶ä»– {len(selected_images) - 10} å¼ å›¾ç‰‡")

        # 8. åˆ›å»ºè‰ç¨¿å¯¹è±¡
        print("\nåˆ›å»ºè‰ç¨¿å¯¹è±¡...")
        draft = self._create_draft_object(
            audio_relative_path,
            image_relative_paths,
            audio_duration_us,
            image_duration_us,
            enable_transitions,
            enable_effects,
            enable_keyframes,
            image_scale,
            srt_file_path  # ä¼ é€’åŸå§‹å­—å¹•æ–‡ä»¶è·¯å¾„ï¼ˆå¯èƒ½ä¸ºNoneï¼‰
        )

        # 9. å¤åˆ¶å…ƒä¿¡æ¯æ–‡ä»¶
        print("\nå¤åˆ¶å…ƒä¿¡æ¯æ–‡ä»¶...")
        self._copy_meta_files(draft_dir)

        # 10. ä¿å­˜è‰ç¨¿
        print("\nä¿å­˜è‰ç¨¿æ–‡ä»¶...")
        self._save_draft(draft, draft_dir)

        # 11. åˆ›å»º ZIP åŒ…
        print("\nåˆ›å»º ZIP åŒ…...")
        zip_path = self._create_zip_package(draft_dir)
        
        # 12. ä¸å†åœ¨æ­¤å¤„ç§»åŠ¨åˆ°æœ¬åœ°å‰ªæ˜ ç›®å½•ï¼Œç”± pipeline_core.py ç»Ÿä¸€å¤„ç†
        # è¿™æ ·é¿å…é‡å¤ç§»åŠ¨çš„é—®é¢˜
        logger.info("è‰ç¨¿ç”Ÿæˆå®Œæˆï¼Œè·³è¿‡æœ¬åœ°ç›®å½•ç§»åŠ¨ï¼ˆç”±pipelineå¤„ç†ï¼‰")
        final_path = draft_dir

        print(f"\n{'=' * 60}")
        safe_print(f"âœ… ç”Ÿæˆå®Œæˆ!")
        print(f"è‰ç¨¿ç›®å½•: {draft_dir}")
        print(f"ZIP åŒ…: {zip_path}")
        print(f"æ€»æ—¶é•¿: {audio_duration_ms / 1000:.2f} ç§’")
        print(f"å›¾ç‰‡æ•°é‡: {len(selected_images)}")
        if enable_transitions:
            print(f"åŒ…å«è½¬åœº: {len(selected_images) - 1} ä¸ª")
        if enable_effects:
            print(f"åŒ…å«ç‰¹æ•ˆ: å·²æ·»åŠ éšæœºç‰¹æ•ˆ")
        if enable_keyframes:
            print(f"åŒ…å«å…³é”®å¸§: å·²æ·»åŠ åŠ¨ç”»æ•ˆæœï¼ˆç¼©æ”¾: {image_scale}xï¼‰")
        print(f"ç”»å¸ƒæ¯”ä¾‹: 16:9 (1920x1080)")
        print(f"\nğŸ“Œ è‰ç¨¿å°†ç”±pipelineç§»åŠ¨åˆ°æœ¬åœ°å‰ªæ˜ ç›®å½•ï¼ˆå¦‚æœé…ç½®ï¼‰")
        print(f"{'=' * 60}\n")

        return final_path

    def _create_draft_object(
            self,
            audio_relative_path: str,
            image_relative_paths: List[str],
            audio_duration_us: int,
            image_duration_us: int,
            enable_transitions: bool,
            enable_effects: bool,
            enable_keyframes: bool = True,  # é»˜è®¤å¯ç”¨
            image_scale: float = 1.8,  # å›¾ç‰‡ç¼©æ”¾æ¯”ä¾‹
            srt_path: Optional[str] = None  # å­—å¹•æ–‡ä»¶è·¯å¾„
    ) -> Draft:
        """åˆ›å»ºè‰ç¨¿å¯¹è±¡ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰"""

        materials = Materials()
        draft_id = gen_upper_uuid()

        # 1. åˆ›å»ºéŸ³é¢‘ç´ æ
        audio_material = AudioMaterial(
            id=gen_upper_uuid(),
            path=audio_relative_path,
            duration=audio_duration_us,
            material_name="èƒŒæ™¯éŸ³ä¹"
        )
        materials.audios = [audio_material.to_dict()]

        # 2. åˆ›å»ºè§†é¢‘ç´ æï¼ˆå›¾ç‰‡ï¼‰
        video_materials = []
        for i, image_path in enumerate(image_relative_paths):
            video_material = VideoMaterial(
                id=gen_upper_uuid(),
                path=image_path,
                duration=image_duration_us,
                width=1920,
                height=1080,
                material_name=f"å›¾ç‰‡{i + 1}"
            )
            video_materials.append(video_material)
            materials.videos.append(video_material.to_dict())

        # 3. åˆ›å»ºè½¬åœºï¼ˆå¦‚æœå¯ç”¨ï¼‰
        transitions = []
        if enable_transitions and len(video_materials) > 1:
            for i in range(len(video_materials) - 1):
                # éšæœºé€‰æ‹©ä¸€ä¸ªè½¬åœº
                transition_info = random.choice(self.available_transitions)
                transition = Transition(
                    id=gen_upper_uuid(),
                    effect_id=transition_info.effect_id,
                    name=transition_info.name,
                    duration=466666,  # çº¦0.47ç§’
                    is_overlap=transition_info.is_overlap,
                    category_id=transition_info.category_id,
                    category_name=transition_info.category_name,
                    path=transition_info.get_path(),
                    resource_id=transition_info.resource_id
                )
                transitions.append(transition)
                materials.transitions.append(transition.to_dict())

        # 4. åˆ›å»ºç‰¹æ•ˆï¼ˆå¦‚æœå¯ç”¨ï¼‰
        video_effect = None
        if enable_effects:
            # éšæœºé€‰æ‹©ä¸€ä¸ªç‰¹æ•ˆ
            effect_info = random.choice(self.available_effects)
            video_effect = VideoEffect(
                id=gen_upper_uuid(),
                effect_id=effect_info.effect_id,
                name=effect_info.name,
                category_id=effect_info.category_id,
                category_name=effect_info.category_name,
                path=effect_info.get_path(),
                resource_id=effect_info.resource_id,
                adjust_params=effect_info.adjust_params,
                value=effect_info.default_value,
                render_index=0,
                track_render_index=0
            )
            materials.video_effects.append(video_effect.to_dict())

        # 5. åˆ›å»ºè¾…åŠ©ç´ æ
        for i in range(len(video_materials)):
            materials.speeds.append(SpeedInfo.create_speed_dict())
            materials.canvases.append(CanvasInfo.create_canvas_dict())
            materials.sound_channel_mappings.append({
                "id": gen_upper_uuid(),
                "audio_channel_mapping": 0,
                "is_config_open": False,
                "type": ""
            })
            materials.vocal_separations.append({
                "id": gen_upper_uuid(),
                "choice": 0,
                "production_path": "",
                "time_range": None,
                "type": "vocal_separation"
            })

        # æ·»åŠ ç´ æåŠ¨ç”»
        for _ in range(len(transitions)):
            materials.material_animations.append(MaterialAnimationInfo.create_animation_dict())

        # 6. åˆ›å»ºè§†é¢‘è½¨é“
        video_track = Track(
            id=gen_upper_uuid(),
            type="video",
            segments=[],
            attribute=1
        )

        # åˆ›å»ºè§†é¢‘ç‰‡æ®µ
        current_time = 0
        for i, video_material in enumerate(video_materials):
            # ç¡®ä¿ä¸è¶…è¿‡éŸ³é¢‘æ—¶é•¿
            if current_time >= audio_duration_us:
                break

            # è®¡ç®—ç‰‡æ®µæ—¶é•¿
            segment_duration = min(image_duration_us, audio_duration_us - current_time)

            # åˆ›å»ºé¢å¤–å¼•ç”¨
            extra_refs = []
            if i < len(materials.speeds):
                extra_refs.append(materials.speeds[i]["id"])
            if enable_transitions and i < len(transitions):
                extra_refs.append(transitions[i].id)
            if i < len(materials.canvases):
                extra_refs.append(materials.canvases[i]["id"])
            if i < len(materials.material_animations):
                extra_refs.append(materials.material_animations[i]["id"])
            if i < len(materials.sound_channel_mappings):
                extra_refs.append(materials.sound_channel_mappings[i]["id"])
            if i < len(materials.vocal_separations):
                extra_refs.append(materials.vocal_separations[i]["id"])

            # åˆ›å»ºç‰‡æ®µ
            video_segment = Segment(
                id=gen_upper_uuid(),
                material_id=video_material.id,
                target=Timerange(start=current_time, duration=segment_duration),
                source=Timerange(start=0, duration=segment_duration),
                speed=1.0,
                volume=0.0,
                clip=Clip(
                    alpha=1.0,
                    rotation=0.0,
                    scale={"x": image_scale if enable_keyframes else 1.0, "y": image_scale if enable_keyframes else 1.0},
                    transform={"x": 0.0, "y": 0.8 if enable_keyframes and i % 2 == 0 else -0.8 if enable_keyframes and i % 2 == 1 else 0.0}
                ),
                extra_refs=extra_refs
            )
            
            # å¦‚æœå¯ç”¨å…³é”®å¸§ï¼Œæ·»åŠ åŠ¨ç”»
            if enable_keyframes:
                video_segment.common_keyframes = self.create_keyframes_for_segment(segment_duration, i, image_scale)
            video_track.segments.append(video_segment)
            current_time += segment_duration

        # 7. åˆ›å»ºéŸ³é¢‘è½¨é“
        audio_track = Track(
            id=gen_upper_uuid(),
            type="audio",
            segments=[],
            attribute=0
        )

        audio_segment = Segment(
            id=gen_upper_uuid(),
            material_id=audio_material.id,
            target=Timerange(start=0, duration=audio_duration_us),
            source=Timerange(start=0, duration=audio_duration_us),
            speed=1.0,
            volume=1.0,
            clip=None,
            extra_refs=[
                materials.speeds[0]["id"] if materials.speeds else gen_upper_uuid(),
                materials.sound_channel_mappings[0]["id"] if materials.sound_channel_mappings else gen_upper_uuid(),
                materials.vocal_separations[0]["id"] if materials.vocal_separations else gen_upper_uuid()
            ]
        )
        audio_track.segments.append(audio_segment)

        # 8. åˆ›å»ºç‰¹æ•ˆè½¨é“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        tracks = [video_track, audio_track]
        if enable_effects and video_effect:
            effect_track = Track(
                id=gen_upper_uuid(),
                type="effect",
                segments=[],
                attribute=0
            )

            effect_segment = Segment(
                id=gen_upper_uuid(),
                material_id=video_effect.id,
                target=Timerange(start=0, duration=audio_duration_us),
                source=Timerange(start=0, duration=0),
                speed=1.0,
                volume=1.0,
                clip=None
            )
            effect_track.segments.append(effect_segment)
            tracks.append(effect_track)

        # 9. å¤„ç†å­—å¹•ï¼ˆå¦‚æœæä¾›äº†SRTæ–‡ä»¶ï¼‰
        if srt_path and os.path.exists(srt_path):
            print("æ·»åŠ å­—å¹•è½¨é“...")
            subtitle_service = get_subtitle_service()
            
            # åˆ›å»ºå­—å¹•ææ–™
            subtitle_materials = subtitle_service.create_subtitle_materials(srt_path)
            
            # æ·»åŠ å­—å¹•ææ–™åˆ°materials
            materials.texts = subtitle_materials['text_materials']
            materials.material_animations.extend(subtitle_materials['animation_materials'])
            
            # åˆ›å»ºå­—å¹•è½¨é“ï¼ˆè½¬æ¢ä¸ºTrackå¯¹è±¡ï¼‰
            subtitle_track_dict = subtitle_service.create_subtitle_track(subtitle_materials['text_segments'])
            
            # è½¬æ¢segmentsä¸ºSegmentå¯¹è±¡
            subtitle_segments = []
            for seg_dict in subtitle_track_dict['segments']:
                # å¤„ç†source_timerangeï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
                source_timerange = seg_dict.get('source_timerange')
                if source_timerange is None:
                    source = Timerange(start=0, duration=0)
                else:
                    source = Timerange(
                        start=source_timerange.get('start', 0),
                        duration=source_timerange.get('duration', 0)
                    )
                
                # å¤„ç†clip
                clip_data = seg_dict.get('clip')
                if clip_data and isinstance(clip_data, dict):
                    clip = Clip(
                        alpha=clip_data.get('alpha', 1.0),
                        rotation=clip_data.get('rotation', 0.0),
                        scale=clip_data.get('scale', {"x": 1.0, "y": 1.0}),
                        transform=clip_data.get('transform', {"x": 0.0, "y": 0.0}),
                        flip=clip_data.get('flip', {"horizontal": False, "vertical": False})
                    )
                else:
                    clip = None
                
                segment = Segment(
                    id=seg_dict['id'],
                    material_id=seg_dict['material_id'],
                    target=Timerange(
                        start=seg_dict['target_timerange']['start'],
                        duration=seg_dict['target_timerange']['duration']
                    ),
                    source=source,
                    speed=seg_dict.get('speed', 1.0),
                    volume=seg_dict.get('volume', 1.0),
                    clip=clip
                )
                # æ·»åŠ å…¶ä»–å¿…è¦çš„å­—æ®µ
                segment.render_index = seg_dict.get('render_index', 14000)
                segment.track_render_index = seg_dict.get('track_render_index', 2)
                segment.track_attribute = seg_dict.get('track_attribute', 0)
                segment.visible = seg_dict.get('visible', True)
                segment.extra_material_refs = seg_dict.get('extra_material_refs', [])
                subtitle_segments.append(segment)
            
            subtitle_track = Track(
                id=subtitle_track_dict['id'],
                type=subtitle_track_dict['type'],
                segments=subtitle_segments,
                attribute=subtitle_track_dict.get('attribute', 0)
            )
            tracks.append(subtitle_track)
            
            print(f"  æ·»åŠ äº† {subtitle_materials['subtitle_count']} æ¡å­—å¹•")
        
        # 10. åˆ›å»ºè‰ç¨¿
        draft = Draft(
            id=draft_id,
            duration=audio_duration_us,
            canvas_config={
                "height": 1080,
                "width": 1920,
                "ratio": "16:9"
            },
            materials=materials,
            tracks=tracks
        )

        return draft

    def _copy_meta_files(self, draft_dir: str):
        """å¤åˆ¶å…ƒä¿¡æ¯æ–‡ä»¶ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰"""
        # è·å–é…ç½®æ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼ˆä¸å½“å‰è„šæœ¬åŒç›®å½•ï¼‰
        config_dir = os.path.dirname(__file__)
        
        # å¤åˆ¶ draft_meta_info.json
        meta_info_src = os.path.join(config_dir, "draft_meta_info.json")
        meta_info_dst = os.path.join(draft_dir, "draft_meta_info.json")
        if os.path.exists(meta_info_src):
            shutil.copy(meta_info_src, meta_info_dst)
            print("å¤åˆ¶å…ƒä¿¡æ¯: draft_meta_info.json")
        else:
            print(f"è­¦å‘Šï¼šæ‰¾ä¸åˆ°å…ƒä¿¡æ¯æ–‡ä»¶: {meta_info_src}")
            # å¦‚æœæ‰¾ä¸åˆ°æ–‡ä»¶ï¼Œåˆ›å»ºä¸€ä¸ªåŸºç¡€çš„
            meta_info = {
                "draft_materials": [],
                "draft_id": gen_upper_uuid(),
                "draft_name": os.path.basename(draft_dir)
            }
            with open(meta_info_dst, 'w', encoding='utf-8') as f:
                json.dump(meta_info, f, indent=4, ensure_ascii=False)
            print("åˆ›å»ºåŸºç¡€å…ƒä¿¡æ¯: draft_meta_info.json")

        # å¤åˆ¶ draft_settings
        settings_src = os.path.join(config_dir, "draft_settings")
        settings_dst = os.path.join(draft_dir, "draft_settings")
        if os.path.exists(settings_src):
            shutil.copy(settings_src, settings_dst)
            print("å¤åˆ¶è®¾ç½®æ–‡ä»¶: draft_settings")
        else:
            print(f"è­¦å‘Šï¼šæ‰¾ä¸åˆ°è®¾ç½®æ–‡ä»¶: {settings_src}")
            # å¦‚æœæ‰¾ä¸åˆ°æ–‡ä»¶ï¼Œåˆ›å»ºä¸€ä¸ªåŸºç¡€çš„
            settings_content = "[General]\n"
            with open(settings_dst, 'w', encoding='utf-8') as f:
                f.write(settings_content)
            print("åˆ›å»ºåŸºç¡€è®¾ç½®æ–‡ä»¶: draft_settings")

    def _move_to_local_dir(self, draft_dir: str, video_title: str) -> Optional[str]:
        """
        ç§»åŠ¨è‰ç¨¿åˆ°æœ¬åœ°å‰ªæ˜ ç›®å½•
        
        Args:
            draft_dir: ç”Ÿæˆçš„è‰ç¨¿ç›®å½•è·¯å¾„
            video_title: è§†é¢‘æ ‡é¢˜ï¼ˆç”¨ä½œç›®æ ‡æ–‡ä»¶å¤¹åï¼‰
        
        Returns:
            ç§»åŠ¨åçš„ç›®æ ‡è·¯å¾„ï¼Œå¦‚æœè·³è¿‡åˆ™è¿”å› None
        """
        # è¯»å–ç¯å¢ƒå˜é‡
        local_dir = os.environ.get('DRAFT_LOCAL_DIR', '').strip()
        
        # å¦‚æœæ˜¯ test æˆ–æœªè®¾ç½®ï¼Œè·³è¿‡ç§»åŠ¨
        if not local_dir or local_dir.lower() == 'test':
            logger.debug(f"DRAFT_LOCAL_DIR æœªè®¾ç½®æˆ–ä¸º testï¼Œè·³è¿‡ç§»åŠ¨")
            return None
        
        # éªŒè¯ç›®æ ‡ç›®å½•
        if not os.path.exists(local_dir):
            logger.warning(f"æœ¬åœ°ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡ç§»åŠ¨: {local_dir}")
            return None
        
        # æ„å»ºç›®æ ‡è·¯å¾„
        target_path = os.path.join(local_dir, video_title)
        
        try:
            # å¦‚æœç›®æ ‡å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
            if os.path.exists(target_path):
                logger.info(f"åˆ é™¤å·²å­˜åœ¨çš„ç›®å½•: {target_path}")
                shutil.rmtree(target_path)
            
            # ç§»åŠ¨æ•´ä¸ªæ–‡ä»¶å¤¹
            shutil.move(draft_dir, target_path)
            logger.info(f"æˆåŠŸç§»åŠ¨åˆ°æœ¬åœ°å‰ªæ˜ ç›®å½•: {target_path}")
            
            return target_path
        except Exception as e:
            logger.error(f"ç§»åŠ¨åˆ°æœ¬åœ°ç›®å½•å¤±è´¥: {str(e)}")
            return None
    
    def _create_zip_package(self, draft_dir: str) -> str:
        """åˆ›å»º ZIP åŒ…ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰"""
        zip_path = f"{draft_dir}.zip"
        
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            # éå†è‰ç¨¿ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
            for root, dirs, files in os.walk(draft_dir):
                for file in files:
                    file_path = os.path.join(root, file)
                    # è®¡ç®—åœ¨ ZIP ä¸­çš„ç›¸å¯¹è·¯å¾„ï¼Œä¸åŒ…å«é¡¶å±‚ç›®å½•å
                    arcname = os.path.relpath(file_path, draft_dir)
                    zipf.write(file_path, arcname)
                    
        # è·å– ZIP æ–‡ä»¶å¤§å°
        zip_size = os.path.getsize(zip_path) / (1024 * 1024)  # è½¬æ¢ä¸º MB
        print(f"ZIP åŒ…åˆ›å»ºå®Œæˆ: {os.path.basename(zip_path)} ({zip_size:.2f} MB)")
        
        return zip_path

    def _save_draft(self, draft: Draft, draft_dir: str):
        """ä¿å­˜è‰ç¨¿ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰"""
        draft_content_path = os.path.join(draft_dir, "draft_content.json")
        draft_dict = draft.to_dict()

        # ç¡®ä¿æ‰€æœ‰è½¨é“çš„segmentæœ‰æ­£ç¡®çš„å­—æ®µ
        for track in draft_dict.get("tracks", []):
            for segment in track.get("segments", []):
                if track["type"] == "video":
                    segment.update({
                        "enable_adjust": True,
                        "enable_color_correct_adjust": False,
                        "enable_color_curves": True,
                        "enable_color_match_adjust": False,
                        "enable_color_wheels": True,
                        "enable_lut": True,
                        "enable_smart_color_adjust": False,
                        "caption_info": None,
                        "cartoon": False,
                        "group_id": "",
                        "hdr_settings": None,
                        "intensifies_audio": False,
                        "is_tone_modify": False,
                        "last_nonzero_volume": 1.0,
                        "render_index": 0,
                        "responsive_layout": {
                            "enable": False,
                            "horizontal_pos_layout": 0,
                            "size_layout": 0,
                            "target_follow": "",
                            "vertical_pos_layout": 0
                        },
                        "track_render_index": 0,
                        "uniform_scale": {
                            "on": True,
                            "value": 1.0
                        }
                    })
                elif track["type"] == "audio":
                    if "clip" not in segment:
                        segment["clip"] = None
                    segment.update({
                        "enable_adjust": False,
                        "enable_color_correct_adjust": False,
                        "enable_color_curves": True,
                        "enable_color_match_adjust": False,
                        "enable_color_wheels": True,
                        "enable_lut": False,
                        "enable_smart_color_adjust": False,
                        "caption_info": None,
                        "cartoon": False,
                        "group_id": "",
                        "hdr_settings": None,
                        "intensifies_audio": False,
                        "is_tone_modify": False,
                        "last_nonzero_volume": 1.0,
                        "render_index": 0,
                        "responsive_layout": {
                            "enable": False,
                            "horizontal_pos_layout": 0,
                            "size_layout": 0,
                            "target_follow": "",
                            "vertical_pos_layout": 0
                        },
                        "track_render_index": 0,
                        "uniform_scale": None
                    })
                elif track["type"] == "effect":
                    segment["source_timerange"] = None
                    segment.update({
                        "render_index": 11000,
                        "track_render_index": 1,
                        "caption_info": None,
                        "cartoon": False,
                        "enable_adjust": False,
                        "enable_color_correct_adjust": False,
                        "enable_color_curves": True,
                        "enable_color_match_adjust": False,
                        "enable_color_wheels": True,
                        "enable_lut": False,
                        "enable_smart_color_adjust": False,
                        "group_id": "",
                        "hdr_settings": None,
                        "intensifies_audio": False,
                        "is_tone_modify": False,
                        "last_nonzero_volume": 1.0,
                        "responsive_layout": {
                            "enable": False,
                            "horizontal_pos_layout": 0,
                            "size_layout": 0,
                            "target_follow": "",
                            "vertical_pos_layout": 0
                        },
                        "uniform_scale": None
                    })

        with open(draft_content_path, 'w', encoding='utf-8') as f:
            json.dump(draft_dict, f, indent=4, ensure_ascii=False)
        print("ä¿å­˜è‰ç¨¿: draft_content.json")


def generate_draft_from_story(cid: str, vid: str, 
                             image_duration_seconds: float = 3.0,
                             enable_transitions: bool = True,
                             enable_effects: bool = True,
                             enable_keyframes: bool = True,  # é»˜è®¤å¯ç”¨
                             image_scale: float = 1.8,  # å›¾ç‰‡ç¼©æ”¾æ¯”ä¾‹
                             random_seed: Optional[int] = 42):
    """
    æ ¹æ®æ•…äº‹IDç”Ÿæˆå‰ªæ˜ è‰ç¨¿
    
    Args:
        cid: Creator ID
        vid: Voice ID
        image_duration_seconds: æ¯å¼ å›¾ç‰‡æ˜¾ç¤ºæ—¶é•¿ï¼ˆç§’ï¼‰
        enable_transitions: æ˜¯å¦å¯ç”¨è½¬åœº
        enable_effects: æ˜¯å¦å¯ç”¨ç‰¹æ•ˆ
        enable_keyframes: æ˜¯å¦å¯ç”¨å…³é”®å¸§åŠ¨ç”»ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
        image_scale: å›¾ç‰‡ç¼©æ”¾æ¯”ä¾‹ï¼ˆé»˜è®¤1.8ï¼‰
        random_seed: éšæœºç§å­
    
    Returns:
        ç”Ÿæˆçš„è‰ç¨¿ç›®å½•è·¯å¾„
    """
    
    # æ„å»ºè·¯å¾„
    audio_path = f"./output/{cid}_{vid}_story.mp3"
    images_dir = "./output/images"
    video_title = f"{cid}_{vid}_story"
    output_dir = "./output/drafts"
    
    # éªŒè¯æ–‡ä»¶å­˜åœ¨
    if not os.path.exists(audio_path):
        raise FileNotFoundError(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
    
    if not os.path.exists(images_dir):
        raise FileNotFoundError(f"å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {images_dir}")
    
    # åˆ›å»ºæœåŠ¡å®ä¾‹
    service = DraftGeneratorService()
    
    try:
        # ç”Ÿæˆè‰ç¨¿
        draft_dir = service.generate_draft(
            images_dir=images_dir,
            audio_path=audio_path,
            image_duration_seconds=image_duration_seconds,
            video_title=video_title,
            output_dir=output_dir,
            enable_transitions=enable_transitions,
            enable_effects=enable_effects,
            enable_keyframes=enable_keyframes,
            image_scale=image_scale,
            random_seed=random_seed
        )
        
        return draft_dir
        
    except Exception as e:
        safe_print(f"\nâŒ ç”Ÿæˆå¤±è´¥: {str(e)}")
        raise


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    
    # åˆ›å»ºå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description='ç”Ÿæˆå‰ªæ˜ è‰ç¨¿')
    parser.add_argument('--cid', type=str, required=True, help='Creator ID')
    parser.add_argument('--vid', type=str, required=True, help='Voice ID')
    parser.add_argument('--duration', type=float, default=3.0, 
                       help='æ¯å¼ å›¾ç‰‡æ˜¾ç¤ºæ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤3ç§’')
    parser.add_argument('--no-transitions', action='store_true',
                       help='ç¦ç”¨è½¬åœºæ•ˆæœ')
    parser.add_argument('--no-effects', action='store_true',
                       help='ç¦ç”¨è§†é¢‘ç‰¹æ•ˆ')
    parser.add_argument('--no-keyframes', action='store_true',
                       help='ç¦ç”¨å…³é”®å¸§åŠ¨ç”»ï¼ˆé»˜è®¤å¯ç”¨ï¼‰')
    parser.add_argument('--scale', type=float, default=1.8,
                       help='å›¾ç‰‡ç¼©æ”¾æ¯”ä¾‹ï¼Œé»˜è®¤1.8')
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­ï¼Œç”¨äºå¤ç°ç»“æœ')
    
    # è§£æå‚æ•°
    args = parser.parse_args()
    
    print(f"\n{'=' * 60}")
    print(f"å‰ªæ˜ è‰ç¨¿ç”Ÿæˆå·¥å…·")
    print(f"{'=' * 60}")
    print(f"Creator ID: {args.cid}")
    print(f"Voice ID: {args.vid}")
    print(f"å›¾ç‰‡æ˜¾ç¤ºæ—¶é•¿: {args.duration} ç§’")
    print(f"è½¬åœºæ•ˆæœ: {'å¯ç”¨' if not args.no_transitions else 'ç¦ç”¨'}")
    print(f"è§†é¢‘ç‰¹æ•ˆ: {'å¯ç”¨' if not args.no_effects else 'ç¦ç”¨'}")
    print(f"å…³é”®å¸§åŠ¨ç”»: {'å¯ç”¨' if not args.no_keyframes else 'ç¦ç”¨'}")
    print(f"å›¾ç‰‡ç¼©æ”¾æ¯”ä¾‹: {args.scale}x")
    print(f"{'=' * 60}\n")
    
    try:
        # ç”Ÿæˆè‰ç¨¿
        draft_dir = generate_draft_from_story(
            cid=args.cid,
            vid=args.vid,
            image_duration_seconds=args.duration,
            enable_transitions=not args.no_transitions,
            enable_effects=not args.no_effects,
            enable_keyframes=not args.no_keyframes,  # æ³¨æ„ï¼šä½¿ç”¨ not args.no_keyframes
            image_scale=args.scale,
            random_seed=args.seed
        )
        
        print(f"\nâœ¨ è‰ç¨¿å·²ç”Ÿæˆåˆ°: {draft_dir}")
        print("\nä¸‹ä¸€æ­¥æ“ä½œï¼š")
        print("1. æ‰“å¼€å‰ªæ˜ ä¸“ä¸šç‰ˆ")
        print("2. é€‰æ‹©'å¯¼å…¥è‰ç¨¿'")
        print("3. é€‰æ‹©ç”Ÿæˆçš„è‰ç¨¿æ–‡ä»¶å¤¹")
        print("4. å¼€å§‹ç¼–è¾‘ä½ çš„è§†é¢‘ï¼")
        
    except FileNotFoundError as e:
        safe_print(f"\nâŒ æ–‡ä»¶æœªæ‰¾åˆ°: {str(e)}")
        print("\nè¯·ç¡®ä¿å·²ç»å®Œæˆä»¥ä¸‹æ­¥éª¤ï¼š")
        print("1. ç”Ÿæˆæ•…äº‹éŸ³é¢‘: python voice_gen/tts_client.py --cid <cid> --vid <vid> --gender <0|1>")
        print("2. ç”Ÿæˆå›¾ç‰‡: python image_generator.py")
        
    except Exception as e:
        safe_print(f"\nâŒ ç”Ÿæˆå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()