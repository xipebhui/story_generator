#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ‰¹é‡YouTubeè§†é¢‘å…ƒä¿¡æ¯æå–å™¨
æ”¯æŒä»è§†é¢‘åˆ—è¡¨æ‰¹é‡æå–å…ƒæ•°æ®å¹¶ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
"""

import os
import sys
import json
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from youtube_metadata_extractor import YouTubeMetadataExtractor

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


class BatchMetadataExtractor:
    """æ‰¹é‡YouTubeè§†é¢‘å…ƒä¿¡æ¯æå–å™¨"""
    
    def __init__(self, output_base_dir: str = "batch_metadata"):
        """
        åˆå§‹åŒ–æ‰¹é‡æå–å™¨
        
        Args:
            output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•
        """
        self.output_base_dir = Path(output_base_dir)
        self.output_base_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºæ‰¹æ¬¡ç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.batch_dir = self.output_base_dir / f"batch_{timestamp}"
        self.batch_dir.mkdir(parents=True, exist_ok=True)
        
        self.results = []
        
        logger.info(f"ğŸ“ æ‰¹æ¬¡è¾“å‡ºç›®å½•: {self.batch_dir}")
    
    def extract_video(self, video_id: str, force_refresh: bool = False) -> Dict[str, Any]:
        """
        æå–å•ä¸ªè§†é¢‘çš„å…ƒæ•°æ®
        
        Args:
            video_id: YouTubeè§†é¢‘ID
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°
            
        Returns:
            æå–ç»“æœ
        """
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ¬ å¤„ç†è§†é¢‘: {video_id}")
        
        result = {
            'video_id': video_id,
            'status': 'pending',
            'error': None,
            'metadata': None
        }
        
        try:
            # åˆ›å»ºæå–å™¨
            video_dir = self.batch_dir / video_id
            extractor = YouTubeMetadataExtractor(
                video_id=video_id,
                output_dir=str(video_dir)
            )
            
            # æå–å…ƒæ•°æ®
            metadata = extractor.extract_all_metadata(force_refresh=force_refresh)
            
            # ç”ŸæˆæŠ¥å‘Š
            report = extractor.generate_summary_report(metadata)
            
            result['status'] = 'success'
            result['metadata'] = metadata
            
            logger.info(f"âœ… è§†é¢‘ {video_id} å¤„ç†æˆåŠŸ")
            
        except Exception as e:
            result['status'] = 'failed'
            result['error'] = str(e)
            logger.error(f"âŒ è§†é¢‘ {video_id} å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        return result
    
    def extract_from_list(self, video_ids: List[str], 
                         force_refresh: bool = False,
                         delay_seconds: int = 2) -> List[Dict[str, Any]]:
        """
        ä»è§†é¢‘IDåˆ—è¡¨æ‰¹é‡æå–
        
        Args:
            video_ids: è§†é¢‘IDåˆ—è¡¨
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°
            delay_seconds: æ¯ä¸ªè§†é¢‘ä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰
            
        Returns:
            æ‰€æœ‰æå–ç»“æœ
        """
        logger.info(f"ğŸ“‹ å¼€å§‹æ‰¹é‡æå– {len(video_ids)} ä¸ªè§†é¢‘")
        
        for i, video_id in enumerate(video_ids, 1):
            logger.info(f"\nè¿›åº¦: {i}/{len(video_ids)}")
            
            result = self.extract_video(video_id, force_refresh)
            self.results.append(result)
            
            # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
            if i < len(video_ids):
                logger.info(f"â¸ï¸ ç­‰å¾… {delay_seconds} ç§’...")
                time.sleep(delay_seconds)
        
        # ä¿å­˜æ‰¹æ¬¡ç»“æœ
        self._save_batch_results()
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        self._generate_batch_report()
        
        return self.results
    
    def extract_from_file(self, file_path: str, 
                         force_refresh: bool = False,
                         delay_seconds: int = 2) -> List[Dict[str, Any]]:
        """
        ä»æ–‡ä»¶è¯»å–è§†é¢‘IDåˆ—è¡¨å¹¶æ‰¹é‡æå–
        
        Args:
            file_path: åŒ…å«è§†é¢‘IDçš„æ–‡ä»¶è·¯å¾„ï¼ˆæ¯è¡Œä¸€ä¸ªIDï¼‰
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°
            delay_seconds: å»¶è¿Ÿç§’æ•°
            
        Returns:
            æ‰€æœ‰æå–ç»“æœ
        """
        video_ids = []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    # æ”¯æŒURLæ ¼å¼
                    if 'youtube.com/watch?v=' in line:
                        video_id = line.split('v=')[1].split('&')[0]
                    elif 'youtu.be/' in line:
                        video_id = line.split('youtu.be/')[1].split('?')[0]
                    else:
                        video_id = line
                    
                    video_ids.append(video_id)
        
        logger.info(f"ğŸ“„ ä»æ–‡ä»¶åŠ è½½äº† {len(video_ids)} ä¸ªè§†é¢‘ID")
        
        return self.extract_from_list(video_ids, force_refresh, delay_seconds)
    
    def _save_batch_results(self):
        """ä¿å­˜æ‰¹æ¬¡ç»“æœ"""
        results_file = self.batch_dir / "batch_results.json"
        
        # å‡†å¤‡ä¿å­˜çš„æ•°æ®ï¼ˆç®€åŒ–ç‰ˆï¼‰
        save_data = []
        for result in self.results:
            item = {
                'video_id': result['video_id'],
                'status': result['status'],
                'error': result['error']
            }
            
            if result['status'] == 'success' and result['metadata']:
                metadata = result['metadata']
                video_info = metadata.get('video_info', {})
                item['title'] = video_info.get('title', 'N/A')
                item['channel'] = video_info.get('channel_title', 'N/A')
                item['views'] = video_info.get('view_count', 0)
                item['likes'] = video_info.get('like_count', 0)
                item['comments_count'] = len(metadata.get('comments', []))
                item['subtitle_length'] = metadata.get('subtitle_length', 0)
            
            save_data.append(item)
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ æ‰¹æ¬¡ç»“æœå·²ä¿å­˜: {results_file}")
    
    def _generate_batch_report(self):
        """ç”Ÿæˆæ‰¹æ¬¡æ±‡æ€»æŠ¥å‘Š"""
        report_file = self.batch_dir / "batch_report.md"
        
        # ç»Ÿè®¡ä¿¡æ¯
        total = len(self.results)
        success = sum(1 for r in self.results if r['status'] == 'success')
        failed = sum(1 for r in self.results if r['status'] == 'failed')
        
        report = f"""# æ‰¹é‡YouTubeè§†é¢‘å…ƒæ•°æ®æå–æŠ¥å‘Š

## ğŸ“Š æ±‡æ€»ç»Ÿè®¡

- **å¤„ç†æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **æ€»è§†é¢‘æ•°**: {total}
- **æˆåŠŸ**: {success}
- **å¤±è´¥**: {failed}
- **æˆåŠŸç‡**: {(success/total*100):.1f}%

## ğŸ“‹ è§†é¢‘åˆ—è¡¨

| # | è§†é¢‘ID | æ ‡é¢˜ | é¢‘é“ | è§‚çœ‹æ•° | ç‚¹èµæ•° | è¯„è®ºæ•° | å­—å¹•é•¿åº¦ | çŠ¶æ€ |
|---|--------|------|------|--------|--------|--------|----------|------|
"""
        
        for i, result in enumerate(self.results, 1):
            video_id = result['video_id']
            status = "âœ…" if result['status'] == 'success' else "âŒ"
            
            if result['status'] == 'success' and result['metadata']:
                metadata = result['metadata']
                video_info = metadata.get('video_info', {})
                title = video_info.get('title', 'N/A')[:30] + '...' if len(video_info.get('title', '')) > 30 else video_info.get('title', 'N/A')
                channel = video_info.get('channel_title', 'N/A')[:20]
                views = f"{video_info.get('view_count', 0):,}"
                likes = f"{video_info.get('like_count', 0):,}"
                comments = len(metadata.get('comments', []))
                subtitle_len = metadata.get('subtitle_length', 0)
                
                report += f"| {i} | {video_id} | {title} | {channel} | {views} | {likes} | {comments} | {subtitle_len} | {status} |\n"
            else:
                error = result.get('error', 'Unknown error')[:30]
                report += f"| {i} | {video_id} | Error: {error} | - | - | - | - | - | {status} |\n"
        
        # æ·»åŠ å¤±è´¥è¯¦æƒ…
        if failed > 0:
            report += "\n## âŒ å¤±è´¥è¯¦æƒ…\n\n"
            for result in self.results:
                if result['status'] == 'failed':
                    report += f"- **{result['video_id']}**: {result.get('error', 'Unknown error')}\n"
        
        # æ·»åŠ çƒ­é—¨è§†é¢‘åˆ†æ
        if success > 0:
            report += "\n## ğŸ”¥ çƒ­é—¨è§†é¢‘ (æŒ‰è§‚çœ‹æ•°)\n\n"
            
            successful_results = [r for r in self.results if r['status'] == 'success' and r['metadata']]
            sorted_results = sorted(
                successful_results,
                key=lambda x: x['metadata']['video_info'].get('view_count', 0),
                reverse=True
            )[:5]
            
            for i, result in enumerate(sorted_results, 1):
                video_info = result['metadata']['video_info']
                report += f"{i}. **{video_info['title'][:50]}**\n"
                report += f"   - é¢‘é“: {video_info['channel_title']}\n"
                report += f"   - è§‚çœ‹: {video_info.get('view_count', 0):,}\n"
                report += f"   - ç‚¹èµ: {video_info.get('like_count', 0):,}\n\n"
        
        report += f"""
## ğŸ“ è¾“å‡ºç›®å½•

æ‰€æœ‰æå–çš„æ•°æ®å·²ä¿å­˜åˆ°:
`{self.batch_dir}`

æ¯ä¸ªè§†é¢‘çš„è¯¦ç»†æ•°æ®åœ¨å„è‡ªçš„å­ç›®å½•ä¸­ã€‚

---
*Generated by Batch YouTube Metadata Extractor*
"""
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"ğŸ“„ æ‰¹æ¬¡æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        
        return report


def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œæ¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ‰¹é‡YouTubeè§†é¢‘å…ƒæ•°æ®æå–å™¨')
    
    # è¾“å…¥æ–¹å¼ï¼ˆäº’æ–¥ï¼‰
    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument('--video-ids', '-v', nargs='+', 
                            help='è§†é¢‘IDåˆ—è¡¨ï¼ˆç©ºæ ¼åˆ†éš”ï¼‰')
    input_group.add_argument('--file', '-f', 
                            help='åŒ…å«è§†é¢‘IDçš„æ–‡ä»¶è·¯å¾„ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰')
    
    parser.add_argument('--output-dir', '-o', default='batch_metadata',
                       help='è¾“å‡ºåŸºç¡€ç›®å½•ï¼ˆé»˜è®¤: batch_metadataï¼‰')
    parser.add_argument('--force-refresh', '-r', action='store_true',
                       help='å¼ºåˆ¶åˆ·æ–°æ‰€æœ‰æ•°æ®')
    parser.add_argument('--delay', '-d', type=int, default=2,
                       help='æ¯ä¸ªè§†é¢‘ä¹‹é—´çš„å»¶è¿Ÿç§’æ•°ï¼ˆé»˜è®¤: 2ï¼‰')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ‰¹é‡æå–å™¨
    extractor = BatchMetadataExtractor(output_base_dir=args.output_dir)
    
    # æ‰§è¡Œæå–
    if args.video_ids:
        results = extractor.extract_from_list(
            video_ids=args.video_ids,
            force_refresh=args.force_refresh,
            delay_seconds=args.delay
        )
    else:
        results = extractor.extract_from_file(
            file_path=args.file,
            force_refresh=args.force_refresh,
            delay_seconds=args.delay
        )
    
    # æ‰“å°æ±‡æ€»
    success = sum(1 for r in results if r['status'] == 'success')
    failed = sum(1 for r in results if r['status'] == 'failed')
    
    print(f"\n{'='*60}")
    print(f"âœ… æ‰¹é‡æå–å®Œæˆï¼")
    print(f"ğŸ“Š æˆåŠŸ: {success}, å¤±è´¥: {failed}")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {extractor.batch_dir}")


if __name__ == "__main__":
    main()