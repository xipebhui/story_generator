#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
YouTubeè§†é¢‘å…ƒä¿¡æ¯æå–å™¨
ç‹¬ç«‹è„šæœ¬ç”¨äºæå–å’Œç¼“å­˜YouTubeè§†é¢‘çš„å…ƒæ•°æ®
åŒ…æ‹¬ï¼šè§†é¢‘è¯¦æƒ…ã€è¯„è®ºã€å­—å¹•ç­‰
"""

import os
import sys
import json
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from youtube_client import YouTubeAPIClient

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(filename)s:%(lineno)d - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


class YouTubeMetadataExtractor:
    """YouTubeè§†é¢‘å…ƒä¿¡æ¯æå–å™¨"""
    
    def __init__(self, video_id: str, output_dir: str = None):
        """
        åˆå§‹åŒ–å…ƒä¿¡æ¯æå–å™¨
        
        Args:
            video_id: YouTubeè§†é¢‘ID
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        """
        self.video_id = video_id
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        if output_dir:
            self.output_dir = Path(output_dir)
        else:
            self.output_dir = Path("metadata") / video_id
        
        # åˆ›å»ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–YouTubeå®¢æˆ·ç«¯
        self._init_youtube_client()
        
        logger.info(f"âœ… åˆå§‹åŒ–å®Œæˆ - è§†é¢‘ID: {video_id}")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def _init_youtube_client(self):
        """åˆå§‹åŒ–YouTube APIå®¢æˆ·ç«¯"""
        # YouTube APIå¯†é’¥
        youtube_api_key = os.getenv("YOUTUBE_API_KEY", "AIzaSyCdbljoACNX1Ov3GsU6KRrnwWnCHAyyjVQ")
        
        self.youtube_client = YouTubeAPIClient()
        logger.info("âœ… YouTubeå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
    
    def extract_video_info(self, force_refresh: bool = False) -> Optional[Dict[str, Any]]:
        """
        æå–è§†é¢‘åŸºæœ¬ä¿¡æ¯
        
        Args:
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
            
        Returns:
            è§†é¢‘ä¿¡æ¯å­—å…¸
        """
        video_info_file = self.output_dir / "video_info.json"
        
        # æ£€æŸ¥ç¼“å­˜
        if not force_refresh and video_info_file.exists():
            logger.info("ğŸ“‚ å‘ç°è§†é¢‘è¯¦æƒ…ç¼“å­˜ï¼Œä»æ–‡ä»¶åŠ è½½...")
            try:
                with open(video_info_file, 'r', encoding='utf-8') as f:
                    video_info = json.load(f)
                logger.info(f"âœ… è§†é¢‘æ ‡é¢˜: {video_info['title']}")
                return video_info
            except Exception as e:
                logger.warning(f"âš ï¸ åŠ è½½è§†é¢‘è¯¦æƒ…å¤±è´¥: {e}")
        
        # ä»YouTubeè·å–
        logger.info("ğŸ“Š ä»YouTubeè·å–è§†é¢‘è¯¦æƒ…...")
        try:
            video_details = self.youtube_client.get_video_details([self.video_id])
            if video_details and video_details.get('items'):
                video_item = video_details['items'][0]
                snippet = video_item['snippet']
                statistics = video_item.get('statistics', {})
                
                video_info = {
                    'video_id': self.video_id,
                    'title': snippet['title'],
                    'description': snippet['description'],
                    'channel_id': snippet['channelId'],
                    'channel_title': snippet['channelTitle'],
                    'published_at': snippet['publishedAt'],
                    'tags': snippet.get('tags', []),
                    'category_id': snippet.get('categoryId'),
                    'duration': video_item.get('contentDetails', {}).get('duration'),
                    'view_count': int(statistics.get('viewCount', 0)),
                    'like_count': int(statistics.get('likeCount', 0)),
                    'comment_count': int(statistics.get('commentCount', 0)),
                    'thumbnail': snippet['thumbnails'].get('maxres', 
                                snippet['thumbnails'].get('high', {})).get('url', ''),
                    'thumbnails': snippet['thumbnails']
                }
                
                # ä¿å­˜åˆ°æ–‡ä»¶
                with open(video_info_file, 'w', encoding='utf-8') as f:
                    json.dump(video_info, f, ensure_ascii=False, indent=2)
                
                logger.info(f"âœ… è§†é¢‘æ ‡é¢˜: {video_info['title']}")
                logger.info(f"ğŸ“Š è§‚çœ‹æ¬¡æ•°: {video_info['view_count']:,}")
                logger.info(f"ğŸ‘ ç‚¹èµæ•°: {video_info['like_count']:,}")
                logger.info(f"ğŸ’¬ è¯„è®ºæ•°: {video_info['comment_count']:,}")
                logger.info(f"ğŸ’¾ è§†é¢‘è¯¦æƒ…å·²ä¿å­˜åˆ°: {video_info_file}")
                
                return video_info
                
        except Exception as e:
            logger.error(f"âŒ è·å–è§†é¢‘è¯¦æƒ…å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def extract_comments(self, max_results: int = 20, force_refresh: bool = False) -> Optional[List[Dict]]:
        """
        æå–è§†é¢‘è¯„è®º
        
        Args:
            max_results: æœ€å¤šè·å–çš„è¯„è®ºæ•°
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°
            
        Returns:
            è¯„è®ºåˆ—è¡¨
        """
        comments_file = self.output_dir / "comments.json"
        
        # æ£€æŸ¥ç¼“å­˜
        if not force_refresh and comments_file.exists():
            logger.info("ğŸ“‚ å‘ç°è¯„è®ºç¼“å­˜ï¼Œä»æ–‡ä»¶åŠ è½½...")
            try:
                with open(comments_file, 'r', encoding='utf-8') as f:
                    comments = json.load(f)
                logger.info(f"âœ… åŠ è½½äº† {len(comments)} æ¡è¯„è®º")
                return comments
            except Exception as e:
                logger.warning(f"âš ï¸ åŠ è½½è¯„è®ºå¤±è´¥: {e}")
        
        # ä»YouTubeè·å–
        logger.info(f"ğŸ’¬ ä»YouTubeè·å–è§†é¢‘è¯„è®ºï¼ˆæœ€å¤š{max_results}æ¡ï¼‰...")
        try:
            comments_data = self.youtube_client.get_video_comments(self.video_id, max_results=max_results)
            if comments_data and comments_data.get('items'):
                comments = []
                for item in comments_data['items']:
                    snippet = item['snippet']['topLevelComment']['snippet']
                    comment_info = {
                        'text_original': snippet['textOriginal']
                    }

                    comments.append(comment_info)

                # ä¿å­˜åˆ°æ–‡ä»¶
                with open(comments_file, 'w', encoding='utf-8') as f:
                    json.dump(comments, f, ensure_ascii=False, indent=2)
                
                logger.info(f"âœ… è·å–äº† {len(comments)} æ¡è¯„è®º")
                logger.info(f"ğŸ”¥ æœ€çƒ­è¯„è®º: {comments[0]['text_original'][:100]}..." if comments else "æ— è¯„è®º")
                logger.info(f"ğŸ’¾ è¯„è®ºå·²ä¿å­˜åˆ°: {comments_file}")
                
                return comments
            else:
                logger.warning("âš ï¸ æœªè·å–åˆ°è¯„è®º")
                return []
                
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–è¯„è®ºå¤±è´¥ï¼ˆéè‡´å‘½ï¼‰: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def extract_subtitle(self, language: str = 'zh', force_refresh: bool = False) -> Optional[str]:
        """
        æå–è§†é¢‘å­—å¹•
        
        Args:
            language: å­—å¹•è¯­è¨€ä»£ç ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™å‚æ•°ä»¥å…¼å®¹ï¼‰
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°
            
        Returns:
            å­—å¹•æ–‡æœ¬
        """
        subtitle_file = self.output_dir / "subtitle.txt"
        
        # æ£€æŸ¥ç¼“å­˜
        if not force_refresh and subtitle_file.exists():
            logger.info(f"ğŸ“‚ å‘ç°å­—å¹•ç¼“å­˜ï¼Œä»æ–‡ä»¶åŠ è½½...")
            try:
                with open(subtitle_file, 'r', encoding='utf-8') as f:
                    subtitle_text = f.read()
                logger.info(f"âœ… åŠ è½½å­—å¹•æˆåŠŸï¼Œé•¿åº¦: {len(subtitle_text)}å­—")
                return subtitle_text
            except Exception as e:
                logger.warning(f"âš ï¸ åŠ è½½å­—å¹•å¤±è´¥: {e}")
        
        # ä»YouTubeè·å–
        logger.info(f"ğŸ“ ä»YouTubeè·å–è§†é¢‘å­—å¹•...")
        try:
            # get_video_transcript ä¸æ¥å— language å‚æ•°
            result = self.youtube_client.get_video_transcript(self.video_id)
            if result:
                # get_video_transcriptè¿”å›çš„æ˜¯å…ƒç»„: (relative_path, subtitle_text)
                relative_path, subtitle_text = result
                
                # ä¿å­˜çº¯æ–‡æœ¬å­—å¹•
                with open(subtitle_file, 'w', encoding='utf-8') as f:
                    f.write(subtitle_text)
                
                logger.info(f"âœ… è·å–å­—å¹•æˆåŠŸï¼Œé•¿åº¦: {len(subtitle_text)}å­—")
                logger.info(f"ğŸ’¾ å­—å¹•å·²ä¿å­˜åˆ°: {subtitle_file}")
                
                return subtitle_text
            else:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°{language}è¯­è¨€å­—å¹•")
                return None
                
        except Exception as e:
            logger.error(f"âŒ è·å–å­—å¹•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def extract_all_metadata(self, force_refresh: bool = False) -> Dict[str, Any]:
        """
        æå–æ‰€æœ‰å…ƒæ•°æ®
        
        Args:
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°æ‰€æœ‰æ•°æ®
            
        Returns:
            åŒ…å«æ‰€æœ‰å…ƒæ•°æ®çš„å­—å…¸
        """
        logger.info(f"ğŸ” å¼€å§‹æå–YouTubeè§†é¢‘å®Œæ•´å…ƒæ•°æ®: {self.video_id}")
        
        metadata = {
            'video_id': self.video_id,
            'video_info': None,
            'comments': [],
            'subtitle': None,
            'extraction_time': None
        }
        
        # 1. æå–è§†é¢‘ä¿¡æ¯
        video_info = self.extract_video_info(force_refresh)
        if video_info:
            metadata['video_info'] = video_info
        
        # 2. æå–è¯„è®º
        comments = self.extract_comments(max_results=20, force_refresh=force_refresh)
        if comments:
            metadata['comments'] = comments
            metadata['top_comments'] = comments[:5]  # å‰5æ¡çƒ­é—¨è¯„è®º
        
        # 3. æå–å­—å¹•
        subtitle = self.extract_subtitle(language='zh', force_refresh=force_refresh)
        if not subtitle:
            # å°è¯•è‹±æ–‡å­—å¹•
            subtitle = self.extract_subtitle(language='en', force_refresh=force_refresh)
        
        if subtitle:
            metadata['subtitle'] = subtitle
            metadata['subtitle_length'] = len(subtitle)
        
        # 4. æ·»åŠ æå–æ—¶é—´
        from datetime import datetime
        metadata['extraction_time'] = datetime.now().isoformat()
        
        # 5. ä¿å­˜å®Œæ•´å…ƒæ•°æ®
        metadata_file = self.output_dir / "metadata_complete.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            # subtitleå¯èƒ½å¾ˆé•¿ï¼Œå•ç‹¬å¤„ç†
            save_data = metadata.copy()
            if 'subtitle' in save_data and save_data['subtitle']:
                save_data['subtitle'] = save_data['subtitle'][:1000] + '...(truncated)'
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        logger.info("âœ… å…ƒæ•°æ®æå–å®Œæˆï¼")
        logger.info(f"ğŸ“Š è§†é¢‘: {metadata['video_info']['title'] if metadata['video_info'] else 'N/A'}")
        logger.info(f"ğŸ’¬ è¯„è®ºæ•°: {len(metadata['comments'])}")
        logger.info(f"ğŸ“ å­—å¹•é•¿åº¦: {metadata.get('subtitle_length', 0)}å­—")
        logger.info(f"ğŸ’¾ å®Œæ•´å…ƒæ•°æ®å·²ä¿å­˜åˆ°: {metadata_file}")
        
        return metadata
    


def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œæ¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description='YouTubeè§†é¢‘å…ƒæ•°æ®æå–å™¨')
    parser.add_argument('video_id', help='YouTubeè§†é¢‘ID')
    parser.add_argument('--output-dir', '-o', help='è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('--force-refresh', '-f', action='store_true', 
                       help='å¼ºåˆ¶åˆ·æ–°æ‰€æœ‰æ•°æ®ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰')
    parser.add_argument('--comments', '-c', type=int, default=20,
                       help='è¦è·å–çš„è¯„è®ºæ•°é‡ï¼ˆé»˜è®¤20ï¼‰')
    parser.add_argument('--language', '-l', default='zh',
                       help='å­—å¹•è¯­è¨€ä»£ç ï¼ˆé»˜è®¤zhï¼‰')
    parser.add_argument('--report', '-r', action='store_true',
                       help='ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæå–å™¨
    extractor = YouTubeMetadataExtractor(
        video_id=args.video_id,
        output_dir=args.output_dir
    )
    
    # æå–æ‰€æœ‰å…ƒæ•°æ®
    metadata = extractor.extract_all_metadata(force_refresh=args.force_refresh)

    
    print(f"\nâœ… å®Œæˆï¼æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ°: {extractor.output_dir}")


if __name__ == "__main__":
    main()