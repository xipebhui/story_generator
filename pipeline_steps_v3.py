#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Pipeline Steps V3 - æ–°ç‰ˆæœ¬çš„Pipelineæ­¥éª¤å®ç°
ä¸¥æ ¼æ¨¡å¼ï¼šä»»ä½•å¤±è´¥éƒ½ä¼šå¯¼è‡´Pipelineç»ˆæ­¢
"""

import json
import re
import sys
import logging
from typing import Dict, Any, List, Optional
from pathlib import Path

from pipeline_architecture import PipelineStep, StepResult
from pipeline_context_v3 import PipelineContextV3
from youtube_client import YouTubeAPIClient
from gemini_client import GeminiClient
from text_processor import TextProcessor
import traceback

logger = logging.getLogger(__name__)


# ============= æ•°æ®è·å–æ­¥éª¤ =============

class FetchYouTubeDataV3Step(PipelineStep):
    """è·å–YouTubeæ•°æ® - V3ç‰ˆæœ¬"""
    
    def __init__(self, youtube_client: YouTubeAPIClient):
        super().__init__("fetch_youtube_data_v3")
        self.youtube_client = youtube_client
    
    def execute(self, context: PipelineContextV3) -> StepResult:
        """æ‰§è¡Œæ•°æ®è·å–"""
        try:
            # æ£€æŸ¥ç¼“å­˜
            if context.cache_dir:
                cached_data = self._load_cached_data(context)
                if cached_data:
                    context.update(**cached_data)
                    logger.info(f"âœ… ä»ç¼“å­˜åŠ è½½YouTubeæ•°æ®")
                    return StepResult(success=True, data=cached_data)
            
            # è·å–è§†é¢‘ä¿¡æ¯
            logger.info(f"ğŸ“Š è·å–è§†é¢‘ä¿¡æ¯: {context.video_id}")
            video_details = self.youtube_client.get_video_details([context.video_id])
            if not video_details or not video_details.get('items'):
                raise Exception(f"Failed to fetch video details for {context.video_id}")
            
            video_info = video_details['items'][0]
            context.video_info = {
                'title': video_info['snippet']['title'],
                'description': video_info['snippet']['description'],
                'channel_title': video_info['snippet']['channelTitle']
            }
            
            # è·å–è¯„è®º
            logger.info("ğŸ’¬ è·å–çƒ­é—¨è¯„è®º")
            comments_data = self.youtube_client.get_video_comments(
                context.video_id, max_results=10
            )
            if comments_data and comments_data.get('items'):
                comments = []
                for item in comments_data['items']:
                    snippet = item['snippet']['topLevelComment']['snippet']
                    comments.append({
                        'text': snippet['textDisplay'],
                        'likes': snippet['likeCount'],
                        'author': snippet['authorDisplayName']
                    })
                context.comments = sorted(
                    comments, key=lambda x: x['likes'], reverse=True
                )[:5]
            
            # è·å–å­—å¹•ï¼ˆå¿…é¡»æˆåŠŸï¼‰
            logger.info("ğŸ“ è·å–è§†é¢‘å­—å¹•")
            subtitle_path = self.youtube_client.get_video_transcript(context.video_id)
            if not subtitle_path:
                raise Exception(f"Failed to fetch transcript for {context.video_id} - TERMINATING")
            
            # è¯»å–å­—å¹•æ–‡ä»¶å†…å®¹
            import os
            absolute_path = os.path.abspath(subtitle_path)
            with open(absolute_path, 'r', encoding='utf-8') as f:
                subtitle_text = f.read()
            context.subtitles = subtitle_text
            
            # ä¿å­˜ç¼“å­˜
            if context.save_intermediate and context.cache_dir:
                self._save_cached_data(context)
            
            logger.info(f"âœ… YouTubeæ•°æ®è·å–æˆåŠŸ")
            return StepResult(success=True)
            
        except Exception as e:
            error_msg = f"Failed to fetch YouTube data: {e}"
            logger.error(f"âŒ {error_msg}")
            context.add_error(error_msg)
            return StepResult(success=False, error=error_msg)
    
    def _load_cached_data(self, context: PipelineContextV3) -> Optional[Dict]:
        """åŠ è½½ç¼“å­˜æ•°æ®"""
        try:
            cache_dir = context.cache_dir / "raw"
            
            data = {}
            
            # åŠ è½½è§†é¢‘ä¿¡æ¯
            video_file = cache_dir / "video_info.json"
            if video_file.exists():
                with open(video_file, 'r', encoding='utf-8') as f:
                    data['video_info'] = json.load(f)
            
            # åŠ è½½è¯„è®º
            comments_file = cache_dir / "comments.json"
            if comments_file.exists():
                with open(comments_file, 'r', encoding='utf-8') as f:
                    data['comments'] = json.load(f)
            
            # åŠ è½½å­—å¹•
            subtitle_file = cache_dir / "subtitle.txt"
            if subtitle_file.exists():
                with open(subtitle_file, 'r', encoding='utf-8') as f:
                    data['subtitles'] = f.read()
            
            # åªæœ‰æ‰€æœ‰å¿…éœ€æ•°æ®éƒ½å­˜åœ¨æ‰è¿”å›ç¼“å­˜
            if data.get('subtitles'):
                return data
                
        except Exception as e:
            logger.warning(f"Failed to load cache: {e}")
        
        return None
    
    def _save_cached_data(self, context: PipelineContextV3):
        """ä¿å­˜ç¼“å­˜æ•°æ®"""
        try:
            cache_dir = context.cache_dir / "raw"
            cache_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜è§†é¢‘ä¿¡æ¯
            with open(cache_dir / "video_info.json", 'w', encoding='utf-8') as f:
                json.dump(context.video_info, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜è¯„è®º
            with open(cache_dir / "comments.json", 'w', encoding='utf-8') as f:
                json.dump(context.comments, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜å­—å¹•
            with open(cache_dir / "subtitle.txt", 'w', encoding='utf-8') as f:
                f.write(context.subtitles)
                
        except Exception as e:
            logger.warning(f"Failed to save cache: {e}")


# ============= Framework V3 ç”Ÿæˆæ­¥éª¤ =============

class GenerateFrameworkV3Step(PipelineStep):
    """ä½¿ç”¨framework_generatorv3.mdç”ŸæˆJSONæ¡†æ¶"""
    
    def __init__(self, gemini_client: GeminiClient):
        super().__init__("generate_framework_v3")
        self.gemini_client = gemini_client
    
    def execute(self, context: PipelineContextV3) -> StepResult:
        """ç”ŸæˆV3æ¡†æ¶"""
        try:
            # æ£€æŸ¥ç¼“å­˜
            if context.cache_dir:
                cached_framework = self._load_cached_framework(context)
                if cached_framework:
                    context.framework_v3_raw = cached_framework['raw']
                    context.framework_v3_json = cached_framework['json']
                    logger.info("âœ… ä»ç¼“å­˜åŠ è½½Framework V3")
                    return StepResult(success=True, data=cached_framework)
            
            # æ„å»ºè¾“å…¥
            input_data = self._build_input(context)
            
            # è·å–æç¤ºè¯
            if not self.prompt_manager:
                raise Exception("Prompt manager not set - TERMINATING")
            
            prompt = self.prompt_manager.get_prompt('framework_generatorv3')
            full_prompt = prompt + "\n" + input_data
            
            logger.info("ğŸ¤– è°ƒç”¨AIç”ŸæˆFramework V3...")
            response = self.gemini_client.generate_content(full_prompt)
            
            if not response:
                raise Exception("Framework V3 generation failed - empty response - TERMINATING")
            
            context.framework_v3_raw = response
            
            # è§£æJSON
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if not json_match:
                raise Exception("Failed to extract JSON from response - TERMINATING")
            
            try:
                context.framework_v3_json = json.loads(json_match.group())
            except json.JSONDecodeError as e:
                raise Exception(f"Failed to parse JSON: {e} - TERMINATING")
            
            # éªŒè¯JSONç»“æ„
            if 'storyBlueprint' not in context.framework_v3_json:
                raise Exception("Invalid JSON structure: missing storyBlueprint - TERMINATING")
            
            # ä¿å­˜ç¼“å­˜
            if context.save_intermediate and context.cache_dir:
                self._save_cached_framework(context)
            
            logger.info(f"âœ… Framework V3ç”ŸæˆæˆåŠŸ")
            return StepResult(success=True)
            
        except Exception as e:
            error_msg = f"Framework V3 generation failed: {e}"
            traceback.print_exc()
            logger.error(f"âŒ {error_msg}")
            context.add_error(error_msg)
            return StepResult(success=False, error=error_msg)
    
    def _build_input(self, context: PipelineContextV3) -> str:
        """æ„å»ºè¾“å…¥æ ¼å¼"""
        comments_text = "\n".join([c['text'] for c in context.comments[:5]])
        
        return f"""[START_OF_INPUT_DATA]
Original Title
{context.video_info.get('title', 'N/A')}

Original Reference Word Count
{len(context.subtitles)}

Hot Comments
{comments_text}

Original Story Text
{context.subtitles}
[END_OF_INPUT_DATA]"""
    
    def _load_cached_framework(self, context: PipelineContextV3) -> Optional[Dict]:
        """åŠ è½½ç¼“å­˜çš„æ¡†æ¶"""
        try:
            cache_file = context.cache_dir / "processing" / "framework_v3.json"
            if cache_file.exists():
                with open(cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                return data
        except Exception as e:
            logger.warning(f"Failed to load cached framework: {e}")
        return None
    
    def _save_cached_framework(self, context: PipelineContextV3):
        """ä¿å­˜æ¡†æ¶ç¼“å­˜"""
        try:
            cache_dir = context.cache_dir / "processing"
            cache_dir.mkdir(parents=True, exist_ok=True)
            
            data = {
                'raw': context.framework_v3_raw,
                'json': context.framework_v3_json
            }
            
            with open(cache_dir / "framework_v3.json", 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logger.warning(f"Failed to save framework cache: {e}")


# ============= æ¡†æ¶è§£ææ­¥éª¤ =============

class ParseFrameworkV3Step(PipelineStep):
    """è§£æV3æ¡†æ¶ï¼Œæå–segmentä¿¡æ¯"""
    
    def __init__(self):
        super().__init__("parse_framework_v3")
    
    def execute(self, context: PipelineContextV3) -> StepResult:
        """è§£ææ¡†æ¶"""
        try:
            if not context.framework_v3_json:
                raise Exception("No framework JSON to parse - TERMINATING")
            
            # æå–storyBlueprint
            blueprint = context.framework_v3_json.get('storyBlueprint', [])
            
            if not blueprint:
                raise Exception("Empty storyBlueprint - TERMINATING")
            
            # è®¾ç½®segmentæ•°é‡å’Œä»»åŠ¡
            context.segment_count = len(blueprint)
            context.segment_tasks = blueprint
            
            logger.info(f"ğŸ“‹ è§£æå‡º {context.segment_count} ä¸ªsegments")
            
            # ä¿å­˜è§£æç»“æœ
            if context.save_intermediate and context.cache_dir:
                self._save_parsed_info(context)
            
            return StepResult(success=True, data={
                'segment_count': context.segment_count,
                'tasks': context.segment_tasks
            })
            
        except Exception as e:
            error_msg = f"Framework parsing failed: {e}"
            logger.error(f"âŒ {error_msg}")
            context.add_error(error_msg)
            return StepResult(success=False, error=error_msg)
    
    def _save_parsed_info(self, context: PipelineContextV3):
        """ä¿å­˜è§£æä¿¡æ¯"""
        try:
            cache_dir = context.cache_dir / "processing"
            cache_dir.mkdir(parents=True, exist_ok=True)
            
            info = {
                'segment_count': context.segment_count,
                'segment_tasks': context.segment_tasks
            }
            
            with open(cache_dir / "parsed_segments.json", 'w', encoding='utf-8') as f:
                json.dump(info, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logger.warning(f"Failed to save parsed info: {e}")


# ============= å¼€å¤´ç”Ÿæˆæ­¥éª¤ =============

class GenerateStoryHeaderStep(PipelineStep):
    """ç”Ÿæˆæ•…äº‹å¼€å¤´"""
    
    def __init__(self, gemini_client: GeminiClient):
        super().__init__("generate_story_header")
        self.gemini_client = gemini_client
    
    def execute(self, context: PipelineContextV3) -> StepResult:
        """ç”Ÿæˆå¼€å¤´"""
        try:
            # æ£€æŸ¥ç¼“å­˜
            if context.cache_dir:
                cached_header = self._load_cached_header(context)
                if cached_header:
                    context.story_header = cached_header
                    logger.info("âœ… ä»ç¼“å­˜åŠ è½½æ•…äº‹å¼€å¤´")
                    return StepResult(success=True, data={'header': cached_header})
            
            # è·å–æç¤ºè¯
            if not self.prompt_manager:
                raise Exception("Prompt manager not set - TERMINATING")
            
            # å‡†å¤‡è¾“å…¥ï¼šå®Œæ•´çš„V3 JSON
            json_input = json.dumps(context.framework_v3_json, indent=2, ensure_ascii=False)
            
            prompt = self.prompt_manager.get_prompt('story_header')
            full_prompt = prompt + "\n\n" + json_input
            
            logger.info("ğŸ¯ ç”Ÿæˆæ•…äº‹å¼€å¤´...")
            response = self.gemini_client.generate_content(full_prompt)
            
            if not response:
                raise Exception("Header generation failed - empty response - TERMINATING")
            
            context.story_header = response
            
            # ä¿å­˜ç¼“å­˜
            if context.save_intermediate and context.cache_dir:
                self._save_cached_header(context)
            
            logger.info(f"âœ… å¼€å¤´ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(response)}å­—ç¬¦")
            return StepResult(success=True)
            
        except Exception as e:
            error_msg = f"Header generation failed: {e}"
            logger.error(f"âŒ {error_msg}")
            context.add_error(error_msg)
            return StepResult(success=False, error=error_msg)
    
    def _load_cached_header(self, context: PipelineContextV3) -> Optional[str]:
        """åŠ è½½ç¼“å­˜çš„å¼€å¤´"""
        try:
            cache_file = context.cache_dir / "processing" / "story_header.txt"
            if cache_file.exists():
                with open(cache_file, 'r', encoding='utf-8') as f:
                    return f.read()
        except Exception as e:
            logger.warning(f"Failed to load cached header: {e}")
        return None
    
    def _save_cached_header(self, context: PipelineContextV3):
        """ä¿å­˜å¼€å¤´ç¼“å­˜"""
        try:
            cache_dir = context.cache_dir / "processing"
            cache_dir.mkdir(parents=True, exist_ok=True)
            
            with open(cache_dir / "story_header.txt", 'w', encoding='utf-8') as f:
                f.write(context.story_header)
                
        except Exception as e:
            logger.warning(f"Failed to save header cache: {e}")


# ============= Segmentç”Ÿæˆæ­¥éª¤ =============

class GenerateAllSegmentsStep(PipelineStep):
    """ç”Ÿæˆæ‰€æœ‰segments"""
    
    def __init__(self, gemini_client: GeminiClient):
        super().__init__("generate_all_segments")
        self.gemini_client = gemini_client
    
    def execute(self, context: PipelineContextV3) -> StepResult:
        """ç”Ÿæˆæ‰€æœ‰segments"""
        try:
            segments = []
            segments_dir = None
            
            if context.cache_dir:
                segments_dir = context.cache_dir / "processing" / "segments"
                segments_dir.mkdir(parents=True, exist_ok=True)
            
            # é€ä¸ªç”Ÿæˆsegment
            for i in range(context.segment_count):
                logger.info(f"ğŸ“ ç”ŸæˆSegment {i+1}/{context.segment_count}")
                
                # æ£€æŸ¥ç¼“å­˜
                cached_segment = None
                if segments_dir:
                    cached_segment = self._load_cached_segment(segments_dir, i+1)
                
                if cached_segment:
                    segments.append(cached_segment)
                    logger.info(f"  âœ… ä»ç¼“å­˜åŠ è½½Segment {i+1}")
                    continue
                
                # ç”Ÿæˆæ–°segment
                segment = self._generate_single_segment(context, i, segments)
                
                if not segment:
                    raise Exception(f"Segment {i+1} generation failed - TERMINATING")
                
                segments.append(segment)
                
                # ä¿å­˜ç¼“å­˜
                if segments_dir:
                    self._save_cached_segment(segments_dir, i+1, segment)
                
                logger.info(f"  âœ… Segment {i+1} ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(segment)}å­—ç¬¦")
            
            context.segments = segments
            logger.info(f"âœ… æ‰€æœ‰ {len(segments)} ä¸ªsegmentsç”Ÿæˆå®Œæˆ")
            
            return StepResult(success=True, data={'segments': segments})
            
        except Exception as e:
            error_msg = f"Segments generation failed: {e}"
            logger.error(f"âŒ {error_msg}")
            context.add_error(error_msg)
            return StepResult(success=False, error=error_msg)
    
    def _generate_single_segment(self, context: PipelineContextV3, 
                                 index: int, 
                                 previous_segments: List[str]) -> Optional[str]:
        """ç”Ÿæˆå•ä¸ªsegment"""
        try:
            # è·å–æç¤ºè¯
            if not self.prompt_manager:
                raise Exception("Prompt manager not set")
            
            # è·å–å‰æ–‡ï¼ˆ200å­—ï¼‰
            if index == 0:
                # ç¬¬ä¸€ä¸ªsegmentï¼Œä½¿ç”¨headerçš„æœ€å200å­—
                previous_text = context.story_header[-200:] if context.story_header else ""
            else:
                # ä½¿ç”¨ä¸Šä¸€ä¸ªsegmentçš„æœ€å200å­—
                previous_text = previous_segments[-1][-200:] if previous_segments else ""
            
            # è·å–å½“å‰stepä¿¡æ¯
            current_task = context.segment_tasks[index]
            
            # æ„å»ºè¾“å…¥
            segment_input = self._build_segment_input(
                context.framework_v3_json,
                index + 1,  # stepç¼–å·ä»1å¼€å§‹
                current_task,
                previous_text
            )
            
            # è·å–segmentæç¤ºè¯å¹¶ç»„åˆ
            segment_prompt = self.prompt_manager.get_prompt('segment_generator')
            full_prompt = segment_prompt + "\n\n" + segment_input
            
            # è°ƒç”¨AIç”Ÿæˆ
            response = self.gemini_client.generate_content(full_prompt)
            
            return response if response else None
            
        except Exception as e:
            logger.error(f"Failed to generate segment {index+1}: {e}")
            return None
    
    def _build_segment_input(self, framework_json: Dict, 
                            step_num: int, 
                            current_task: Dict,
                            previous_text: str) -> str:
        """æ„å»ºsegmentç”Ÿæˆçš„è¾“å…¥"""
        # å‡†å¤‡æ¡†æ¶JSON
        framework_str = json.dumps(framework_json, indent=2, ensure_ascii=False)
        
        # å‡†å¤‡å½“å‰ä»»åŠ¡ä¿¡æ¯
        task_str = json.dumps(current_task, indent=2, ensure_ascii=False)
        
        return f"""==================================================
FRAMEWORK V3 (Complete JSON)
==================================================
{framework_str}

==================================================
CURRENT STEP NUMBER
==================================================
Step {step_num}

==================================================
CURRENT STEP TASK
==================================================
{task_str}

==================================================
PREVIOUS TEXT (Last 200 characters)
==================================================
{previous_text if previous_text else "[This is the first segment after header]"}
"""
    
    def _load_cached_segment(self, segments_dir: Path, segment_num: int) -> Optional[str]:
        """åŠ è½½ç¼“å­˜çš„segment"""
        try:
            cache_file = segments_dir / f"segment_{segment_num:02d}.txt"
            if cache_file.exists():
                with open(cache_file, 'r', encoding='utf-8') as f:
                    return f.read()
        except Exception as e:
            logger.warning(f"Failed to load cached segment {segment_num}: {e}")
        return None
    
    def _save_cached_segment(self, segments_dir: Path, segment_num: int, content: str):
        """ä¿å­˜segmentç¼“å­˜"""
        try:
            cache_file = segments_dir / f"segment_{segment_num:02d}.txt"
            with open(cache_file, 'w', encoding='utf-8') as f:
                f.write(content)
        except Exception as e:
            logger.warning(f"Failed to save segment {segment_num}: {e}")


# ============= æ‹¼æ¥å’Œæ¶¦è‰²æ­¥éª¤ =============

class MergeAndPolishStep(PipelineStep):
    """æ‹¼æ¥æ‰€æœ‰å†…å®¹å¹¶æ¶¦è‰²"""
    
    def __init__(self, gemini_client: GeminiClient):
        super().__init__("merge_and_polish")
        self.gemini_client = gemini_client
        self.text_processor = TextProcessor()
    
    def execute(self, context: PipelineContextV3) -> StepResult:
        """æ‰§è¡Œæ‹¼æ¥å’Œæ¶¦è‰²"""
        try:
            # æ‹¼æ¥æ‰€æœ‰å†…å®¹
            logger.info("ğŸ”— æ‹¼æ¥æ•…äº‹å†…å®¹...")
            merged_parts = [context.story_header] + context.segments
            context.merged_story = "\n\n".join(merged_parts)
            
            # ä¿å­˜æ‹¼æ¥ç»“æœ
            if context.save_intermediate and context.cache_dir:
                cache_dir = context.cache_dir / "processing"
                cache_dir.mkdir(parents=True, exist_ok=True)
                with open(cache_dir / "merged_story.txt", 'w', encoding='utf-8') as f:
                    f.write(context.merged_story)
            
            logger.info(f"  âœ… æ‹¼æ¥å®Œæˆï¼Œæ€»é•¿åº¦: {len(context.merged_story)}å­—ç¬¦")
            
            # æ£€æŸ¥æ¶¦è‰²ç¼“å­˜
            if context.cache_dir:
                cached_polish = self._load_cached_polish(context)
                if cached_polish:
                    context.polished_story = cached_polish
                    context.final_story = cached_polish
                    logger.info("âœ… ä»ç¼“å­˜åŠ è½½æ¶¦è‰²ç»“æœ")
                    return StepResult(success=True)
            
            # æ‰§è¡Œæ¶¦è‰²
            logger.info("âœ¨ å¼€å§‹æ¶¦è‰²æ•…äº‹...")
            
            # è·å–æ¶¦è‰²æç¤ºè¯
            if not self.prompt_manager:
                raise Exception("Prompt manager not set - TERMINATING")
            
            # æ„å»ºæ¶¦è‰²è¾“å…¥
            polish_input = self.text_processor.format_polish_input(
                json.dumps(context.framework_v3_json, indent=2, ensure_ascii=False),
                context.merged_story,
                num_segments=context.segment_count
            )
            
            polish_prompt = self.prompt_manager.get_prompt('final_polisher')
            full_prompt = polish_prompt + "\n\n" + polish_input
            
            response = self.gemini_client.generate_content(full_prompt)
            
            if not response:
                # æ¶¦è‰²å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ‹¼æ¥ç‰ˆæœ¬
                logger.warning("âš ï¸ æ¶¦è‰²å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ç‰ˆæœ¬")
                context.polished_story = context.merged_story
                context.final_story = context.merged_story
            else:
                # è§£ææ¶¦è‰²ç»“æœ
                polish_result = self.text_processor.parse_polish_output(response)
                context.polished_story = polish_result.get('story', context.merged_story)
                context.final_story = context.polished_story
                
                # ä¿å­˜ç¼–è¾‘æŠ¥å‘Š
                if 'report' in polish_result and context.cache_dir:
                    report_file = context.cache_dir / "processing" / "polish_report.txt"
                    with open(report_file, 'w', encoding='utf-8') as f:
                        f.write(polish_result['report'])
            
            # ä¿å­˜æ¶¦è‰²ç»“æœ
            if context.save_intermediate and context.cache_dir:
                self._save_cached_polish(context)
            
            logger.info(f"âœ… æ¶¦è‰²å®Œæˆï¼Œæœ€ç»ˆé•¿åº¦: {len(context.final_story)}å­—ç¬¦")
            return StepResult(success=True)
            
        except Exception as e:
            # æ¶¦è‰²å¤±è´¥ä¸ç»ˆæ­¢ï¼Œä½¿ç”¨åŸå§‹ç‰ˆæœ¬
            logger.warning(f"Polish failed, using raw version: {e}")
            context.final_story = context.merged_story if context.merged_story else ""
            return StepResult(success=True)  # è¿”å›æˆåŠŸï¼Œç»§ç»­æµç¨‹
    
    def _load_cached_polish(self, context: PipelineContextV3) -> Optional[str]:
        """åŠ è½½ç¼“å­˜çš„æ¶¦è‰²ç»“æœ"""
        try:
            cache_file = context.cache_dir / "processing" / "polished_story.txt"
            if cache_file.exists():
                with open(cache_file, 'r', encoding='utf-8') as f:
                    return f.read()
        except Exception as e:
            logger.warning(f"Failed to load cached polish: {e}")
        return None
    
    def _save_cached_polish(self, context: PipelineContextV3):
        """ä¿å­˜æ¶¦è‰²ç»“æœ"""
        try:
            cache_dir = context.cache_dir / "processing"
            with open(cache_dir / "polished_story.txt", 'w', encoding='utf-8') as f:
                f.write(context.polished_story)
        except Exception as e:
            logger.warning(f"Failed to save polish cache: {e}")


# ============= æ€»ç»“ç”Ÿæˆæ­¥éª¤ =============

class GenerateSummaryStep(PipelineStep):
    """ç”Ÿæˆä¸­æ–‡æ€»ç»“"""
    
    def __init__(self, gemini_client: GeminiClient):
        super().__init__("generate_summary")
        self.gemini_client = gemini_client
    
    def execute(self, context: PipelineContextV3) -> StepResult:
        """ç”Ÿæˆæ€»ç»“"""
        try:
            if not context.final_story:
                logger.warning("No story to summarize")
                context.summary_cn = "æ— æ•…äº‹å†…å®¹"
                return StepResult(success=True)
            
            logger.info("ğŸ“ ç”Ÿæˆä¸­æ–‡æ€»ç»“...")
            
            # ç®€å•çš„æ€»ç»“æç¤ºè¯
            summary_prompt = """è¯·ä¸ºä»¥ä¸‹è‹±æ–‡æ•…äº‹ç”Ÿæˆä¸€ä»½è¯¦ç»†çš„ä¸­æ–‡æ€»ç»“ã€‚

è¦æ±‚ï¼š
1. æ€»ç»“æ•…äº‹çš„ä¸»è¦æƒ…èŠ‚ï¼ˆ500å­—å·¦å³ï¼‰
2. åˆ†ææ•…äº‹çš„æ ¸å¿ƒä¸»é¢˜å’Œä»·å€¼è§‚
3. åˆ—å‡ºä¸»è¦è§’è‰²åŠå…¶ç‰¹ç‚¹
4. è¯„ä»·æ•…äº‹çš„äº®ç‚¹å’Œç‰¹è‰²
5. æä¾›YouTubeå‘å¸ƒå»ºè®®ï¼ˆæ ‡é¢˜ã€æ ‡ç­¾ã€å°é¢ç­‰ï¼‰

æ•…äº‹å†…å®¹ï¼š
"""
            
            # åªä½¿ç”¨æ•…äº‹çš„å‰5000å­—è¿›è¡Œæ€»ç»“ï¼ˆé¿å…tokené™åˆ¶ï¼‰
            story_excerpt = context.final_story[:5000] if len(context.final_story) > 5000 else context.final_story
            
            full_prompt = summary_prompt + story_excerpt
            
            response = self.gemini_client.generate_content(full_prompt)
            
            if response:
                context.summary_cn = response
                logger.info("âœ… ä¸­æ–‡æ€»ç»“ç”ŸæˆæˆåŠŸ")
            else:
                context.summary_cn = "æ€»ç»“ç”Ÿæˆå¤±è´¥"
                logger.warning("âš ï¸ æ€»ç»“ç”Ÿæˆå¤±è´¥")
            
            # ä¿å­˜æ€»ç»“
            if context.save_intermediate and context.cache_dir:
                self._save_summary(context)
            
            return StepResult(success=True)
            
        except Exception as e:
            # æ€»ç»“å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            logger.warning(f"Summary generation failed: {e}")
            context.summary_cn = f"æ€»ç»“ç”Ÿæˆå¤±è´¥: {str(e)}"
            return StepResult(success=True)
    
    def _save_summary(self, context: PipelineContextV3):
        """ä¿å­˜æ€»ç»“"""
        try:
            final_dir = context.cache_dir / "final"
            final_dir.mkdir(parents=True, exist_ok=True)
            
            with open(final_dir / "summary_cn.txt", 'w', encoding='utf-8') as f:
                f.write(context.summary_cn)
                
        except Exception as e:
            logger.warning(f"Failed to save summary: {e}")


# ============= æœ€ç»ˆè¾“å‡ºæ­¥éª¤ =============

class SaveFinalOutputStep(PipelineStep):
    """ä¿å­˜æœ€ç»ˆè¾“å‡ºæ–‡ä»¶"""
    
    def __init__(self):
        super().__init__("save_final_output")
    
    def execute(self, context: PipelineContextV3) -> StepResult:
        """ä¿å­˜æ‰€æœ‰æœ€ç»ˆæ–‡ä»¶"""
        try:
            if not context.cache_dir:
                logger.warning("No cache directory specified")
                return StepResult(success=True)
            
            final_dir = context.cache_dir / "final"
            final_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜æœ€ç»ˆæ•…äº‹
            with open(final_dir / "story.txt", 'w', encoding='utf-8') as f:
                f.write(context.final_story)
            
            # ä¿å­˜å…ƒæ•°æ®
            metadata = {
                'video_id': context.video_id,
                'creator_name': context.creator_name,
                'video_title': context.video_info.get('title', 'N/A'),
                'segment_count': context.segment_count,
                'story_length': len(context.final_story),
                'header_length': len(context.story_header),
                'errors': context.errors
            }
            
            with open(final_dir / "metadata.json", 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            # ç”ŸæˆæŠ¥å‘Š
            report = self._generate_report(context)
            with open(final_dir / "report.md", 'w', encoding='utf-8') as f:
                f.write(report)
            
            logger.info(f"âœ… æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ°: {final_dir}")
            
            return StepResult(success=True)
            
        except Exception as e:
            error_msg = f"Failed to save final output: {e}"
            logger.error(error_msg)
            return StepResult(success=False, error=error_msg)
    
    def _generate_report(self, context: PipelineContextV3) -> str:
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        from datetime import datetime
        
        return f"""# V3 Pipeline æ‰§è¡ŒæŠ¥å‘Š

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## åŸºæœ¬ä¿¡æ¯
- **è§†é¢‘ID**: {context.video_id}
- **åˆ›ä½œè€…**: {context.creator_name}
- **è§†é¢‘æ ‡é¢˜**: {context.video_info.get('title', 'N/A')}

## ç”Ÿæˆç»Ÿè®¡
- **Segmentæ•°é‡**: {context.segment_count}
- **å¼€å¤´é•¿åº¦**: {len(context.story_header)} å­—ç¬¦
- **åŸå§‹æ‹¼æ¥é•¿åº¦**: {len(context.merged_story)} å­—ç¬¦
- **æœ€ç»ˆæ•…äº‹é•¿åº¦**: {len(context.final_story)} å­—ç¬¦

## é”™è¯¯è®°å½•
{chr(10).join(context.errors) if context.errors else 'æ— é”™è¯¯'}

## ä¸­æ–‡æ€»ç»“
{context.summary_cn}

## YouTubeå‘å¸ƒå»ºè®®
{ 'å·²ç”ŸæˆåŒè¯­ç‰ˆæœ¬ - è¯·æŸ¥çœ‹youtube_metadata.md' if context.youtube_metadata else 'æœªç”Ÿæˆ'}

## æ•…äº‹é¢„è§ˆ

### å¼€å¤´ï¼ˆå‰500å­—ç¬¦ï¼‰
```
{context.final_story[:500] if context.final_story else 'N/A'}
```

### ç»“å°¾ï¼ˆå500å­—ç¬¦ï¼‰
```
{context.final_story[-500:] if context.final_story else 'N/A'}
```

---
*æŠ¥å‘Šç”Ÿæˆå®Œæ¯•*
"""